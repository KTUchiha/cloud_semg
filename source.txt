./README.md
---
# Cloud-Based Personalized sEMG Hand Gesture Classification System

This project implements a cloud-based personalized surface electromyography (sEMG) hand gesture classification system using Convolutional Neural Networks (CNNs). The system allows each user to create a profile, define a custom gesture bank, and train a personalized CNN model that adapts to their unique muscle signal patterns. It leverages a centralized server architecture to manage data, training, and real-time inference while keeping the hardware requirements on the client side minimal.

---

## Overview

- **Personalization:** Each user’s model is trained on their own sEMG data, enabling higher accuracy by adapting to individual variations in muscle signals.
- **Cloud-Based Architecture:** All processing—including training and inference—is performed on a centralized server. This facilitates scalability and easy access to user profiles from any device.
- **Real-Time Gesture Recognition:** sEMG sensor data are collected and transmitted in real time via UDP. The system processes the data to deliver real-time gesture predictions.
- **Lightweight Models:** With a storage footprint of approximately 1 MB per model, the system is efficient and scalable.

---

## System Components

Below are the various System Components 

![System Architecture](/system_architecture.png "System Architecture")

### Machine Learning Modules

- **Training Module (`./ml_model/TrainMLJob.py`):**  
  Retrieves training data from a PostgreSQL database, preprocesses it (scaling, encoding, and sequence generation), trains a CNN model, and stores the resulting model artifacts (including sensor configuration and class mapping).

- **API Module (`./ml_model/api.py`):**  
  A FastAPI application that loads the latest model artifacts for each user, preprocesses incoming sensor data, and returns gesture predictions via the `/predict` endpoint.

- **NATS Inference Module (`./ml_model/nat_inference.py`):**  
  Subscribes to a NATS topic, processes batches of sensor data, and logs API responses in the database.

### User Interface and Data Management (Streamlit Apps)

- **Main App (`./streamlit_apps/Main.py`):**  
  Serves as the entry point for the web-based user interface.

- **User Management (`./streamlit_apps/pages/1_User_Management.py`):**  
  Provides functionality to create, update, or delete user profiles in the PostgreSQL database.

- **Gesture Management (`./streamlit_apps/pages/2_Gesture_Management.py`):**  
  Manages user-specific gesture definitions and sensor usage settings.

- **Training Data Capture (`./streamlit_apps/pages/3_Training_Data_Capture.py`):**  
  Captures synchronized sensor data and video streams for labeling and training.

- **Kickoff Training (`./streamlit_apps/pages/4_Kickoff_Training.py`):**  
  Allows reviewing training sessions, scheduling training jobs, and visualizing sensor data and video recordings.

- **Training Job Status (`./streamlit_apps/pages/5_Training_Jobs_Status.py`):**  
  Monitors the status, logs, and errors of ongoing training jobs.

- **Real-Time Gesture Recognition (`./streamlit_apps/pages/6_Realtime_Gesture_Recognition.py`):**  
  Displays real-time sensor data and gesture predictions.

- **Sensor Data Review (`./streamlit_apps/pages/7_Sensor_Data_Review.py`):**  
  Provides tools to review raw sensor and video data for debugging and quality control.

### Data Simulation, Testing, and Utilities

- **UDP Data Simulator (`./testing/udpsim.py`):**  
  Simulates the transmission of sEMG sensor data via UDP packets for testing purposes.

- **Database Viewer (`./db/view_dbs.py`):**  
  Inspects SQLite database files to view table schemas and sample data.

- **UDP Server (`./udpserver/udpserver.py`):**  
  Receives UDP packets, stores sensor data in PostgreSQL, and publishes messages to a NATS topic.

- **Sample Data Loader (`./testing/sampledataload.py`):**  
  Generates and inserts synthetic sensor data into the database for testing.

- **Arduino Data Simulator (`./testing/simulate_arduinodata.py`):**  
  Mimics sensor data output from an Arduino-based system.

- **Test Inference Script (`./testing/test_inference.py`):**  
  Fetches sensor data from the database in batches and sends them to the prediction API for testing.

### Shell Scripts

- **`start_all.sh`:**  
  Starts the NATS server, Streamlit app, UDP server, and ML model server (FastAPI and NATS inference) in the background, with logs saved to a dedicated logs directory.

- **`stop_all.sh`:**  
  Stops all running services (NATS server, Streamlit app, UDP server, and ML model server).

- **`requirements.txt`:**
  python requirements.txt file to install the packages.

### Database Scripts

- **`./db/postgres.sql`:**  
  Contains the PostgreSQL schema (tables for users, sensor data, gestures, training metadata, etc.) with the necessary foreign keys and constraints.


## Setup and Installation

### Prerequisites

- **Python 3.8+**
- **PostgreSQL** – To store user profiles, sensor data, training metadata, and model artifacts.
- **NATS Server** – For real-time Pub/Sub messaging.
- **Streamlit** – For the web user interface.
- **PyTorch** – For CNN model training.
- **FastAPI & Uvicorn** – For serving the inference API.
- Additional Python libraries: `pandas`, `numpy`, `scikit-learn`, `joblib`, `fire`, `psycopg2`, `uvicorn`, etc.
- **Streamlit-WebRTC** - For realtime data collection through video

### Installation Steps

1. **Clone the Repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2. **Create and Activate a Virtual Environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3. **Install Required Packages:**
    ```bash
    pip install -r requirements.txt
    ```

4. **Database Setup:**
   - Create a PostgreSQL database named `sensordb`.
   - Adjust the database connection parameters in the source code files (host, database, user, and password).
   - Run the provided SQL script to set up the necessary tables:
     ```bash
     psql -U <your_username> -d sensordb -f ./db/postgres.sql
     ```
5. **Database Schema Design:**
    ![Database Schema Diagram](db/PSEMG.png "Database Schema Diagram")


### Installing and Configuring WebRTC (streamlit-webrtc)

1. **Install streamlit-webrtc:**
   - Add the following line to your `requirements.txt`:
     ```
     streamlit-webrtc
     ```
   - Then install with:
     ```bash
     pip install -r requirements.txt
     ```

2. **Install FFmpeg:**
   - **Ubuntu/Debian:**
     ```bash
     sudo apt-get update && sudo apt-get install ffmpeg
     ```
   - **macOS (Homebrew):**
     ```bash
     brew install ffmpeg
     ```
   - **Windows:**
     Download FFmpeg from [ffmpeg.org](https://ffmpeg.org/download.html) or install via Chocolatey:
     ```bash
     choco install ffmpeg
     ```

3. **Additional Configuration:**
   - For advanced configuration and troubleshooting, please follow the detailed directions at [streamlit-webrtc on GitHub](https://github.com/whitphx/streamlit-webrtc).

---

## Variables to Customize

Before running or deploying the system, review and update the following variables in the source code:

1. **Database Connection Parameters**  
   These settings appear in multiple files (e.g., `./ml_model/TrainMLJob.py`, `./ml_model/api.py`, `./udpserver/udpserver.py`, and various Streamlit pages). Update these values as follows:
   - **Host:**  
     Typically set to `localhost` if your PostgreSQL server is running locally. Otherwise, set it to your remote database host.
   - **Database:**  
     Set this to the name of your database (e.g., `sensordb`).
   - **User:**  
     Replace placeholder values (e.g., `"XXXXX"`) with your PostgreSQL username.
   - **Password:**  
     Replace placeholder values (e.g., `"XXXXX"`) with your PostgreSQL password.

2. **NATS Server Configuration**  
   In files such as `./ml_model/nat_inference.py` and `./udpserver/udpserver.py`, customize the following:
   - **NATS_SERVER:**  
     The URL of your NATS server (e.g., `nats://127.0.0.1:4222`).
   - **NATS_USER:**  
     Replace placeholder `"XXXXXXX"` with your NATS username.
   - **NATS_PASSWORD:**  
     Replace placeholder `"XXXXXXX"` with your NATS password.

3. **Project Directory**  
   In the shell scripts (`start_all.sh` and `stop_all.sh`), update the following variable:
   - **PROJECT_DIR:**  
     Set this to the absolute path of your project directory.  
     *Example:*  
     ```bash
     PROJECT_DIR=/home/yourusername/emg
     ```

4. **Port and Service Settings**  
   - **API Server Port:**  
     The FastAPI server is started on port `8000` (via uvicorn in `start_all.sh`). Ensure this port is available or change it if necessary.
   - **UDP Server Port:**  
     Typically set to `8081` in `./udpserver/udpserver.py` and referenced in testing scripts.
   - **Streamlit Default Port:**  
     By default, Streamlit runs on port `8501` unless otherwise configured.

5. **Sensor Data Simulation Settings**  
   In testing scripts (e.g., `./testing/simulate_arduinodata.py` and `./testing/udpsim.py`):
   - **DEST_IP:**  
     Set this to the IP address of your UDP server if it is not running locally.
   - **USER_ID:**  
     Ensure that the simulated sensor data uses the correct user ID for testing purposes.

Make sure to update these settings in the corresponding files before starting the system or deploying to a production environment.

-----

## Running the System by each Component

1. **Start the Inference API Server:**
    ```bash
    cd ml_model
    uvicorn api:app --host 0.0.0.0 --port 8000
    ```

2. **Launch the UDP Server:**
    ```bash
    cd udpserver
    python udpserver.py
    ```

3. **Run the Streamlit User Interface:**
    ```bash
    cd streamlit_apps
    streamlit run Main.py
    ```

4. **Simulate Sensor Data (for Testing):**
    ```bash
    cd testing
    python udpsim.py
    ```

---

## Starting and Stopping the System 

### Starting the System

To start all services, run the `start_all.sh` script from the project’s root directory. This script:

- Creates a `logs` directory (if it doesn't exist).
- Starts the **NATS server** (for real-time Pub/Sub messaging) using a config file.
- Launches the **Streamlit** user interface for managing the system.
- Starts the **UDP server** to receive sensor data.
- Launches the **ML Model Server**, which includes:
  - A FastAPI application serving the `/predict` endpoint.
  - A NATS inference process for handling sensor data batches.

**Command:**
```bash 
./start_all.sh
```

### Stopping the System

To stop all running services, run the `stop_all.sh` script. This script terminates the processes for the NATS server, Streamlit app, UDP server, and ML Model Server.

**Command:**
```bash
./stop_all.sh
```


## Training and Inference

- **Training:**  
  Initiate a training job from the "Kickoff Training" page in the Streamlit UI. The training module (`TrainMLJob.py`) will retrieve the labeled sensor data and video metadata from the database, train the CNN, and store the model artifacts.

- **Inference:**  
  The FastAPI-based inference module loads the latest model artifacts for each user and exposes a `/predict` endpoint to accept real-time sensor data. Predictions are displayed in the real-time recognition UI.

---

## Project Structure

.
├── README.md
├── ml_model
│   ├── TrainMLJob.py        # Script for training the CNN model
│   ├── api.py               # FastAPI application for real-time inference
│   └── nat_inference.py     # NATS-based inference module
├── streamlit_apps
│   ├── Main.py              # Main Streamlit entry point
│   └── pages
│       ├── 1_User_Management.py
│       ├── 2_Gesture_Management.py
│       ├── 3_Training_Data_Capture.py
│       ├── 4_Kickoff_Training.py
│       ├── 5_Training_Jobs_Status.py
│       ├── 6_Realtime_Gesture_Recognition.py
│       └── 7_Sensor_Data_Review.py
├── testing
│   └── udpsim.py            # UDP data simulator for testing
├── db
│   └── view_dbs.py          # Utility for viewing database details
├── udpserver
│   └── udpserver.py         # UDP server for receiving and publishing sensor data
└── tmp
    ├── sampledataload.py    # Sample data generation and insertion script
    ├── simulate_arduinodata.py  # Arduino data simulation script
    └── test_inference.py    # Script for testing the inference API



---

## Related Research

This system is based on the research paper:  
**A Cloud-Based Personalized sEMG Hand Gesture Classification System Using Convolutional Neural Networks**  
by Kaavya Tatavarty, Maxwell Johnson, and Boris Rubinsky.  
For more details, please refer to the accompanying PDF document.

---

## Acknowledgments

 Special thanks to the University of California, Berkeley for the support provided during the development of this project and my mentor Maxwell Johnson and Professor Boris Rubinsky.

---

## License

MIT License

Copyright (c) [2023] [Kaavya Tatavarty]

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
**Use at your own risk. This is a research prototype provided without any warranty.**


---
## Known Issues and Limitations

**Prototype Status:**  
This system is a research prototype. Some features may be incomplete or unoptimized.

**Performance Variability:**  
Real-time performance depends on hardware and network conditions.

**Signal Variability:**  
While personalized models improve accuracy, extreme variability in sEMG signals between users may still challenge the system.

**Credential Management:**  
Some scripts may contain hard-coded credentials for testing purposes; ensure they are secured before production deployment.

## Security Considerations

**Sensitive Data:**  
Avoid hard-coding credentials in your source code. Use environment variables or secure configuration management.

**Network Security:**  
Use HTTPS and secure WebSocket connections for all remote communications.

**Database Security:**  
Restrict database access via firewalls and enforce proper user permissions.

**Data Privacy:**  
Ensure that any personally identifiable information (PII) is anonymized or encrypted in accordance with relevant data protection regulations.




## Contact

For questions or support, please contact:  
- **Email:** kaavya.tatavarty@gmail.com



---
./requirements.txt
---
psycopg2
pandas
numpy
torch
scikit-learn
joblib
fire
fastapi
pydantic
uvicorn
requests
streamlit
streamlit-webrtc
av
opencv-python
plotly
scipy
aiosqlite
nats-py
streamlit-webrtc

---
./source.txt
---
./README.md
---
# Cloud-Based Personalized sEMG Hand Gesture Classification System

This project implements a cloud-based personalized surface electromyography (sEMG) hand gesture classification system using Convolutional Neural Networks (CNNs). The system allows each user to create a profile, define a custom gesture bank, and train a personalized CNN model that adapts to their unique muscle signal patterns. It leverages a centralized server architecture to manage data, training, and real-time inference while keeping the hardware requirements on the client side minimal.

---

## Overview

- **Personalization:** Each user’s model is trained on their own sEMG data, enabling higher accuracy by adapting to individual variations in muscle signals.
- **Cloud-Based Architecture:** All processing—including training and inference—is performed on a centralized server. This facilitates scalability and easy access to user profiles from any device.
- **Real-Time Gesture Recognition:** sEMG sensor data are collected and transmitted in real time via UDP. The system processes the data to deliver real-time gesture predictions.
- **Lightweight Models:** With a storage footprint of approximately 1 MB per model, the system is efficient and scalable.

---

## System Components

Below are the various System Components 

![System Architecture](/system_architecture.png "System Architecture")

### Machine Learning Modules

- **Training Module (`./ml_model/TrainMLJob.py`):**  
  Retrieves training data from a PostgreSQL database, preprocesses it (scaling, encoding, and sequence generation), trains a CNN model, and stores the resulting model artifacts (including sensor configuration and class mapping).

- **API Module (`./ml_model/api.py`):**  
  A FastAPI application that loads the latest model artifacts for each user, preprocesses incoming sensor data, and returns gesture predictions via the `/predict` endpoint.

- **NATS Inference Module (`./ml_model/nat_inference.py`):**  
  Subscribes to a NATS topic, processes batches of sensor data, and logs API responses in the database.

### User Interface and Data Management (Streamlit Apps)

- **Main App (`./streamlit_apps/Main.py`):**  
  Serves as the entry point for the web-based user interface.

- **User Management (`./streamlit_apps/pages/1_User_Management.py`):**  
  Provides functionality to create, update, or delete user profiles in the PostgreSQL database.

- **Gesture Management (`./streamlit_apps/pages/2_Gesture_Management.py`):**  
  Manages user-specific gesture definitions and sensor usage settings.

- **Training Data Capture (`./streamlit_apps/pages/3_Training_Data_Capture.py`):**  
  Captures synchronized sensor data and video streams for labeling and training.

- **Kickoff Training (`./streamlit_apps/pages/4_Kickoff_Training.py`):**  
  Allows reviewing training sessions, scheduling training jobs, and visualizing sensor data and video recordings.

- **Training Job Status (`./streamlit_apps/pages/5_Training_Jobs_Status.py`):**  
  Monitors the status, logs, and errors of ongoing training jobs.

- **Real-Time Gesture Recognition (`./streamlit_apps/pages/6_Realtime_Gesture_Recognition.py`):**  
  Displays real-time sensor data and gesture predictions.

- **Sensor Data Review (`./streamlit_apps/pages/7_Sensor_Data_Review.py`):**  
  Provides tools to review raw sensor and video data for debugging and quality control.

### Data Simulation, Testing, and Utilities

- **UDP Data Simulator (`./testing/udpsim.py`):**  
  Simulates the transmission of sEMG sensor data via UDP packets for testing purposes.

- **Database Viewer (`./db/view_dbs.py`):**  
  Inspects SQLite database files to view table schemas and sample data.

- **UDP Server (`./udpserver/udpserver.py`):**  
  Receives UDP packets, stores sensor data in PostgreSQL, and publishes messages to a NATS topic.

- **Sample Data Loader (`./testing/sampledataload.py`):**  
  Generates and inserts synthetic sensor data into the database for testing.

- **Arduino Data Simulator (`./testing/simulate_arduinodata.py`):**  
  Mimics sensor data output from an Arduino-based system.

- **Test Inference Script (`./testing/test_inference.py`):**  
  Fetches sensor data from the database in batches and sends them to the prediction API for testing.

### Shell Scripts

- **`start_all.sh`:**  
  Starts the NATS server, Streamlit app, UDP server, and ML model server (FastAPI and NATS inference) in the background, with logs saved to a dedicated logs directory.

- **`stop_all.sh`:**  
  Stops all running services (NATS server, Streamlit app, UDP server, and ML model server).

- **`requirements.txt`:**
  python requirements.txt file to install the packages.

### Database Scripts

- **`./db/postgres.sql`:**  
  Contains the PostgreSQL schema (tables for users, sensor data, gestures, training metadata, etc.) with the necessary foreign keys and constraints.


## Setup and Installation

### Prerequisites

- **Python 3.8+**
- **PostgreSQL** – To store user profiles, sensor data, training metadata, and model artifacts.
- **NATS Server** – For real-time Pub/Sub messaging.
- **Streamlit** – For the web user interface.
- **PyTorch** – For CNN model training.
- **FastAPI & Uvicorn** – For serving the inference API.
- Additional Python libraries: `pandas`, `numpy`, `scikit-learn`, `joblib`, `fire`, `psycopg2`, `uvicorn`, etc.
- **Streamlit-WebRTC** - For realtime data collection through video

### Installation Steps

1. **Clone the Repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2. **Create and Activate a Virtual Environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3. **Install Required Packages:**
    ```bash
    pip install -r requirements.txt
    ```

4. **Database Setup:**
   - Create a PostgreSQL database named `sensordb`.
   - Adjust the database connection parameters in the source code files (host, database, user, and password).
   - Run the provided SQL script to set up the necessary tables:
     ```bash
     psql -U <your_username> -d sensordb -f ./db/postgres.sql
     ```
5. **Database Schema Design:**
    ![Database Schema Diagram](db/PSEMG.png "Database Schema Diagram")


### Installing and Configuring WebRTC (streamlit-webrtc)

1. **Install streamlit-webrtc:**
   - Add the following line to your `requirements.txt`:
     ```
     streamlit-webrtc
     ```
   - Then install with:
     ```bash
     pip install -r requirements.txt
     ```

2. **Install FFmpeg:**
   - **Ubuntu/Debian:**
     ```bash
     sudo apt-get update && sudo apt-get install ffmpeg
     ```
   - **macOS (Homebrew):**
     ```bash
     brew install ffmpeg
     ```
   - **Windows:**
     Download FFmpeg from [ffmpeg.org](https://ffmpeg.org/download.html) or install via Chocolatey:
     ```bash
     choco install ffmpeg
     ```

3. **Additional Configuration:**
   - For advanced configuration and troubleshooting, please follow the detailed directions at [streamlit-webrtc on GitHub](https://github.com/whitphx/streamlit-webrtc).

---

## Variables to Customize

Before running or deploying the system, review and update the following variables in the source code:

1. **Database Connection Parameters**  
   These settings appear in multiple files (e.g., `./ml_model/TrainMLJob.py`, `./ml_model/api.py`, `./udpserver/udpserver.py`, and various Streamlit pages). Update these values as follows:
   - **Host:**  
     Typically set to `localhost` if your PostgreSQL server is running locally. Otherwise, set it to your remote database host.
   - **Database:**  
     Set this to the name of your database (e.g., `sensordb`).
   - **User:**  
     Replace placeholder values (e.g., `"XXXXX"`) with your PostgreSQL username.
   - **Password:**  
     Replace placeholder values (e.g., `"XXXXX"`) with your PostgreSQL password.

2. **NATS Server Configuration**  
   In files such as `./ml_model/nat_inference.py` and `./udpserver/udpserver.py`, customize the following:
   - **NATS_SERVER:**  
     The URL of your NATS server (e.g., `nats://127.0.0.1:4222`).
   - **NATS_USER:**  
     Replace placeholder `"XXXXXXX"` with your NATS username.
   - **NATS_PASSWORD:**  
     Replace placeholder `"XXXXXXX"` with your NATS password.

3. **Project Directory**  
   In the shell scripts (`start_all.sh` and `stop_all.sh`), update the following variable:
   - **PROJECT_DIR:**  
     Set this to the absolute path of your project directory.  
     *Example:*  
     ```bash
     PROJECT_DIR=/home/yourusername/emg
     ```

4. **Port and Service Settings**  
   - **API Server Port:**  
     The FastAPI server is started on port `8000` (via uvicorn in `start_all.sh`). Ensure this port is available or change it if necessary.
   - **UDP Server Port:**  
     Typically set to `8081` in `./udpserver/udpserver.py` and referenced in testing scripts.
   - **Streamlit Default Port:**  
     By default, Streamlit runs on port `8501` unless otherwise configured.

5. **Sensor Data Simulation Settings**  
   In testing scripts (e.g., `./testing/simulate_arduinodata.py` and `./testing/udpsim.py`):
   - **DEST_IP:**  
     Set this to the IP address of your UDP server if it is not running locally.
   - **USER_ID:**  
     Ensure that the simulated sensor data uses the correct user ID for testing purposes.

Make sure to update these settings in the corresponding files before starting the system or deploying to a production environment.

-----

## Running the System by each Component

1. **Start the Inference API Server:**
    ```bash
    cd ml_model
    uvicorn api:app --host 0.0.0.0 --port 8000
    ```

2. **Launch the UDP Server:**
    ```bash
    cd udpserver
    python udpserver.py
    ```

3. **Run the Streamlit User Interface:**
    ```bash
    cd streamlit_apps
    streamlit run Main.py
    ```

4. **Simulate Sensor Data (for Testing):**
    ```bash
    cd testing
    python udpsim.py
    ```

---

## Starting and Stopping the System 

### Starting the System

To start all services, run the `start_all.sh` script from the project’s root directory. This script:

- Creates a `logs` directory (if it doesn't exist).
- Starts the **NATS server** (for real-time Pub/Sub messaging) using a config file.
- Launches the **Streamlit** user interface for managing the system.
- Starts the **UDP server** to receive sensor data.
- Launches the **ML Model Server**, which includes:
  - A FastAPI application serving the `/predict` endpoint.
  - A NATS inference process for handling sensor data batches.

**Command:**
```bash 
./start_all.sh
```

### Stopping the System

To stop all running services, run the `stop_all.sh` script. This script terminates the processes for the NATS server, Streamlit app, UDP server, and ML Model Server.

**Command:**
```bash
./stop_all.sh
```


## Training and Inference

- **Training:**  
  Initiate a training job from the "Kickoff Training" page in the Streamlit UI. The training module (`TrainMLJob.py`) will retrieve the labeled sensor data and video metadata from the database, train the CNN, and store the model artifacts.

- **Inference:**  
  The FastAPI-based inference module loads the latest model artifacts for each user and exposes a `/predict` endpoint to accept real-time sensor data. Predictions are displayed in the real-time recognition UI.

---

## Project Structure

.
├── README.md
├── ml_model
│   ├── TrainMLJob.py        # Script for training the CNN model
│   ├── api.py               # FastAPI application for real-time inference
│   └── nat_inference.py     # NATS-based inference module
├── streamlit_apps
│   ├── Main.py              # Main Streamlit entry point
│   └── pages
│       ├── 1_User_Management.py
│       ├── 2_Gesture_Management.py
│       ├── 3_Training_Data_Capture.py
│       ├── 4_Kickoff_Training.py
│       ├── 5_Training_Jobs_Status.py
│       ├── 6_Realtime_Gesture_Recognition.py
│       └── 7_Sensor_Data_Review.py
├── testing
│   └── udpsim.py            # UDP data simulator for testing
├── db
│   └── view_dbs.py          # Utility for viewing database details
├── udpserver
│   └── udpserver.py         # UDP server for receiving and publishing sensor data
└── tmp
    ├── sampledataload.py    # Sample data generation and insertion script
    ├── simulate_arduinodata.py  # Arduino data simulation script
    └── test_inference.py    # Script for testing the inference API



---

## Related Research

This system is based on the research paper:  
**A Cloud-Based Personalized sEMG Hand Gesture Classification System Using Convolutional Neural Networks**  
by Kaavya Tatavarty, Maxwell Johnson, and Boris Rubinsky.  
For more details, please refer to the accompanying PDF document.

---

## Acknowledgments

 Special thanks to the University of California, Berkeley for the support provided during the development of this project and my mentor Maxwell Johnson and Professor Boris Rubinsky.

---

## License

MIT License

Copyright (c) [2023] [Kaavya Tatavarty]

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
**Use at your own risk. This is a research prototype provided without any warranty.**


---
## Known Issues and Limitations

**Prototype Status:**  
This system is a research prototype. Some features may be incomplete or unoptimized.

**Performance Variability:**  
Real-time performance depends on hardware and network conditions.

**Signal Variability:**  
While personalized models improve accuracy, extreme variability in sEMG signals between users may still challenge the system.

**Credential Management:**  
Some scripts may contain hard-coded credentials for testing purposes; ensure they are secured before production deployment.

## Security Considerations

**Sensitive Data:**  
Avoid hard-coding credentials in your source code. Use environment variables or secure configuration management.

**Network Security:**  
Use HTTPS and secure WebSocket connections for all remote communications.

**Database Security:**  
Restrict database access via firewalls and enforce proper user permissions.

**Data Privacy:**  
Ensure that any personally identifiable information (PII) is anonymized or encrypted in accordance with relevant data protection regulations.




## Contact

For questions or support, please contact:  
- **Email:** kaavya.tatavarty@gmail.com



---
./requirements.txt
---
psycopg2
pandas
numpy
torch
scikit-learn
joblib
fire
fastapi
pydantic
uvicorn
requests
streamlit
streamlit-webrtc
av
opencv-python
plotly
scipy
aiosqlite
nats-py
streamlit-webrtc

---


---
./start_all.sh
---
#!/bin/bash
PROJECT_DIR=/home/kaavya/emg
LOGS=$PROJECT_DIR/logs
mkdir -p $LOGS
echo "Starting NatServer"
cd ${PROJECT_DIR}/natserver
nohup ${PROJECT_DIR}/natserver/nats-server -c nats-server.conf 2>&1> $LOGS/natserver.log   & 
echo "Starting Streamlit App"
cd ${PROJECT_DIR}/streamlit_apps
nohup streamlit run Main.py 2>&1> $LOGS/streamlit.log & 
echo "Starting UDP Server"
cd ${PROJECT_DIR}/udpserver
nohup python3 udpserver.py 2>&1> $LOGS/udpserver.log & 
echo "Starting ML Model Server"
cd $PROJECT_DIR/ml_model
nohup uvicorn api:app --reload --host='0.0.0.0' --port=8000 2>&1> $LOGS/api.logs & 
nohup python3 nat_inference.py 2>&1> $LOGS/nat_inference.logs & 



---
./stop_all.sh
---
#!/bin/bash
PROJECT_DIR=/home/kaavya/emg
LOGS=$PROJECT_DIR/logs

echo "Stopping NatServer"
natserver_pid=$(ps aux | grep '[n]ats-server' | awk '{print $2}')
if [ ! -z "$natserver_pid" ]; then
  kill $natserver_pid
  echo "NatServer stopped."
else
  echo "NatServer not running."
fi

echo "Stopping Streamlit App"
streamlit_pid=$(ps aux | grep '[s]treamlit' | awk '{print $2}')
if [ ! -z "$streamlit_pid" ]; then
  echo "killing $streamlit_pid"
	kill $streamlit_pid
  echo "Streamlit App stopped."
else
  echo "Streamlit App not running."
fi

echo "Stopping UDP Server"
udpserver_pid=$(ps aux | grep '[u]dpserver.py' | awk '{print $2}')
if [ ! -z "$udpserver_pid" ]; then
  kill $udpserver_pid
  echo "UDP Server stopped."
else
  echo "UDP Server not running."
fi

echo "Stopping ML Model Server"
ml_model_pid=$(ps aux | grep '[u]vicorn' | awk '{print $2}')
if [ ! -z "$ml_model_pid" ]; then
  kill $ml_model_pid
  echo "ML Model Server stopped."
else
  echo "ML Model Server not running."
fi

nat_inference_pid=$(ps aux | grep '[n]at_inference.py' | awk '{print $2}')
if [ ! -z "$nat_inference_pid" ]; then
  kill $nat_inference_pid
  echo "NAT Inference stopped."
else
  echo "NAT Inference not running."
fi

echo "All processes stopped."


---
./ml_model/TrainMLJob.py
---
import psycopg2
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
import joblib
import os
import torch.nn.functional as F
import json
from datetime import datetime
import traceback
import pickle
import fire

def get_training_job_data(job_id):
    conn = psycopg2.connect(
        host="localhost",
        database="sensordb",  # Replace with your database name
        user="XXXXX",          # Replace with your username
        password="XXXXX"           # Replace with your password
    )
    
    cursor = conn.cursor()

    gesture_sensor_query = f"""
    SELECT 
        ugt.gestureid, 
        ugt.sensor_a0_used, 
        ugt.sensor_a1_used, 
        ugt.sensor_a2_used, 
        ugt.sensor_a3_used, 
        ugt.sensor_a4_used
    FROM 
        user_gestures ugt
    WHERE 
        ugt.gestureid IN (
            SELECT DISTINCT gestureid
            FROM user_gesture_trainingmetadata ugtm
            JOIN job_training_metadata jtm ON ugtm.training_metadata_id = jtm.training_metadata_id
            WHERE jtm.job_id = {job_id}
        );
    """

    cursor.execute(gesture_sensor_query)
    gesture_sensors = cursor.fetchall()

    sensors_used = {
        'sensor_a0': any(row[1] for row in gesture_sensors),
        'sensor_a1': any(row[2] for row in gesture_sensors),
        'sensor_a2': any(row[3] for row in gesture_sensors),
        'sensor_a3': any(row[4] for row in gesture_sensors),
        'sensor_a4': any(row[5] for row in gesture_sensors)
    }

    columns = ['timestamp']
    selected_sensors = []
    for sensor, used in sensors_used.items():
        if used:
            selected_sensors.append(sensor)
            columns.append(sensor.replace('sensor_', 'Sensor '))
    columns.append('gesture_id')

    data_query = f"""
    WITH training_metadata AS (
        SELECT 
            start_time, 
            end_time, 
            sample_number,
            CASE 
                WHEN status = 'REST PERIOD' THEN 'REST PERIOD' 
                ELSE (select gesture_description from user_gestures u where u.gestureid = ugt.gestureid ) 
            END AS gesture_id
        FROM 
            user_gesture_trainingmetadata ugt  
        WHERE 
            ugt.training_metadata_id IN (
                SELECT training_metadata_id 
                FROM job_training_metadata
                WHERE job_id = {job_id}
            )
    )
    SELECT 
        us.ts AS timestamp, 
        {', '.join(selected_sensors)},
        tmd.gesture_id  
    FROM 
        user_sensor us, 
        training_metadata tmd
    WHERE 
        us.userid = (SELECT userid FROM training_job_schedule WHERE job_id = {job_id}) 
        AND us.ts BETWEEN tmd.start_time AND tmd.end_time
    ORDER BY 
        us.ts;
    """

    cursor.execute(data_query)
    results = cursor.fetchall()
    df = pd.DataFrame(results, columns=columns)

    cursor.close()
    conn.close()

    return df, sensors_used


# Preprocessing the data (scaling and encoding)
def preprocess_data(df, sensor_columns):
    scaler = StandardScaler()
    scaler.fit(df[sensor_columns].values)
    df[sensor_columns] = scaler.transform(df[sensor_columns].values)
    
    df['gesture_id'] = df['gesture_id'].astype('category')
    class_mapping = dict(enumerate(df['gesture_id'].cat.categories))
    df['gesture_id'] = df['gesture_id'].cat.codes
    
    return df, class_mapping, scaler



# Preparing the data for training
class EMGDataset(torch.utils.data.Dataset):
    def __init__(self, df, sequence_length=32):
        num_sensors = df.shape[1] - 2  # Exclude timestamp and target columns
        self.sequences, self.labels = self.process_df(df, sequence_length, num_sensors)
        
        # Convert to PyTorch tensors
        try:
            self.sequences = torch.tensor(self.sequences, dtype=torch.float32)
            self.labels = torch.tensor(self.labels, dtype=torch.long)
        except Exception as e:
            print(f"Error converting to tensor: {e}")
            print("Sequences type:", self.sequences.dtype)
            print("Labels type:", self.labels.dtype)
            print("First few sequences:", self.sequences[:5])
            print("First few labels:", self.labels[:5])
            raise e

    def process_df(self, rdf, sequence_length, num_sensors):
        sequences = []
        labels = []
        for row in range(sequence_length, rdf.shape[0]):
            sequence = rdf.iloc[row-sequence_length:row, 1:1+num_sensors].values.T.flatten()  # Exclude timestamp
            target = rdf.iloc[row, -1]  # Target column (gesture_id)
            sequences.append(sequence)
            labels.append(target)
        return np.array(sequences), np.array(labels)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]


# Defining the CNN model
class EnhancedAudioCNN(nn.Module):
    def __init__(self, num_classes):
        super(EnhancedAudioCNN, self).__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)
        self.residual_conv = nn.Conv1d(1, 64, kernel_size=1, stride=4, padding=0)
        self.fc_input_size = self._initialize_fc_input_size()
        self.fc1 = nn.Linear(self.fc_input_size, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, num_classes)

    def _initialize_fc_input_size(self):
        with torch.no_grad():
            x = torch.zeros(1, 1, 128)  # Adjust the size according to your data
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            return x.numel()

    def forward(self, x):
        residual = self.residual_conv(x)
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        if residual.size(2) != x.size(2):
            residual = F.adaptive_avg_pool1d(residual, x.size(2))
        x += residual
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x



# Function to store artifacts in the database
def store_artifacts_in_db(job_id, userid, model, class_mapping, scaler, sensors_used, conn):
    # Serialize the model and scaler
    model_blob = pickle.dumps(model.state_dict())
    scaler_blob = pickle.dumps(scaler)
    
    # Convert sensors used and class mapping to JSON
    sensors_used_json = json.dumps(sensors_used)
    class_mapping_json = json.dumps(class_mapping)
    
    # Insert or update the artifacts in the database
    cursor = conn.cursor()
    query = """
    INSERT INTO public.training_job_artifacts (job_id, userid, model, class_mapping, scaler, sensors_used)
    VALUES (%s, %s, %s, %s, %s, %s)
    ON CONFLICT (job_id)
    DO UPDATE SET
        model = EXCLUDED.model,
        class_mapping = EXCLUDED.class_mapping,
        scaler = EXCLUDED.scaler,
        sensors_used = EXCLUDED.sensors_used,
        userid = EXCLUDED.userid;
    """
    cursor.execute(query, (job_id, userid, model_blob, class_mapping_json, scaler_blob, sensors_used_json))
    conn.commit()
    cursor.close()


def update_job_status(job_id, status=None, log_message=None, error_message=None, conn=None):
    """Update the status, log messages, or error messages for the job."""
    update_fields = []
    update_values = []

    if status:
        update_fields.append("job_status = %s")
        update_values.append(status)

    if log_message:
        update_fields.append("log_messages = COALESCE(log_messages, '') || %s || '\n'")
        update_values.append(log_message)

    if error_message:
        update_fields.append("error_message = %s")
        update_values.append(error_message)

    if status == 'in-progress' and 'actual_start_time' not in update_fields:
        update_fields.append("actual_start_time = %s")
        update_values.append(datetime.now())

    if status in ('completed', 'failed', 'canceled'):
        update_fields.append("actual_end_time = %s")
        update_values.append(datetime.now())

    if update_fields:
        query = f"UPDATE training_job_schedule SET {', '.join(update_fields)} WHERE job_id = %s"
        update_values.append(job_id)

        cursor = conn.cursor()
        cursor.execute(query, update_values)
        conn.commit()
        cursor.close()

def train_model(job_id, num_epochs=50, sequence_length=32, batch_size=32):
    # Initialize database connection
    conn = psycopg2.connect(
        host="localhost",
        database="sensordb",  # Replace with your database name
        user="kaavya",          # Replace with your username
        password="sEMG1234"           # Replace with your password
    )

    try:
        # Set job status to in-progress
        update_job_status(job_id, status='in-progress', log_message='Training started.', conn=conn)
        df, sensors_used = get_training_job_data(job_id)
        
        # Preprocess data and get class mapping
        sensor_columns = df.columns[1:-1]  # All columns except the last one (assumed to be the target)
        df, class_mapping, scaler = preprocess_data(df, sensor_columns)
        
        # Create dataset
        dataset = EMGDataset(df, sequence_length=sequence_length)
        
        # Split the dataset into training and validation sets
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
        
        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # Model instantiation
        num_classes = df['gesture_id'].nunique()
        model = EnhancedAudioCNN(num_classes=num_classes)
        
        # Move the model to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        
        # Loss and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=3e-4)
        
        # Compile the model for better performance
        model = torch.compile(model)
        
        for epoch in range(num_epochs):
            model.train()
            running_loss = 0.0
            correct_predictions = 0
            total_predictions = 0
            for batch_idx, (inputs, labels) in enumerate(train_dataloader):
                inputs, labels = inputs.to(device), labels.to(device)
                inputs = inputs.unsqueeze(1)  # Add a channel dimension
                
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct_predictions += (predicted == labels).sum().item()
                total_predictions += labels.size(0)

                # Log progress every 10 batches
                if (batch_idx + 1) % 10 == 0:
                    log_msg = (f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], '
                               f'Loss: {running_loss/(batch_idx+1):.4f}, '
                               f'Accuracy: {correct_predictions/total_predictions:.4f}')
                    update_job_status(job_id, log_message=log_msg, conn=conn)
            
            # Validation phase
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            with torch.no_grad():
                for inputs, labels in val_dataloader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    inputs = inputs.unsqueeze(1)  # Add a channel dimension
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs, 1)
                    val_correct += (predicted == labels).sum().item()
                    val_total += labels.size(0)
            
            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_dataloader):.4f}, '
                  f'Validation Loss: {val_loss/len(val_dataloader):.4f}, '
                  f'Validation Accuracy: {val_correct/val_total:.4f}')
        
        # Get the userid for the job from the training_job_schedule table
        cursor = conn.cursor()
        cursor.execute("SELECT userid FROM training_job_schedule WHERE job_id = %s", (job_id,))
        userid = cursor.fetchone()[0]
        cursor.close()

        # Save artifacts to the database
        store_artifacts_in_db(job_id, userid, model, class_mapping, scaler, sensors_used, conn)

        # Update job status to completed
        update_job_status(job_id, status='completed', log_message='Training completed successfully.', conn=conn)
        print(f'Training complete and artifacts saved to the database.')
    
    except Exception as e:
        error_message = str(e)
        stack_trace = traceback.format_exc()  # Get the full stack trace
        full_error_message = f"{error_message}\n\nStack Trace:\n{stack_trace}"
        
        update_job_status(job_id, status='failed', log_message='Training failed.', error_message=full_error_message, conn=conn)
        print(f'Error occurred: {full_error_message}')
    
    finally:
        conn.close()

# Example usage
if __name__ == '__main__':
  fire.Fire(train_model)



---
./ml_model/api.py
---
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import joblib
import psycopg2
import json
import pickle

app = FastAPI()

# Define input data schema
class InputData(BaseModel):
    userid: int
    data: list

# Define your model architecture
class EnhancedAudioCNN(nn.Module):
    def __init__(self, num_classes):
        super(EnhancedAudioCNN, self).__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)
        self.residual_conv = nn.Conv1d(1, 64, kernel_size=1, stride=4, padding=0)
        self.fc_input_size = self._initialize_fc_input_size()
        self.fc1 = nn.Linear(self.fc_input_size, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, num_classes)

    def _initialize_fc_input_size(self):
        with torch.no_grad():
            x = torch.zeros(1, 1, 128)
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            return x.numel()

    def forward(self, x):
        residual = self.residual_conv(x)
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        if residual.size(2) != x.size(2):
            residual = F.adaptive_avg_pool1d(residual, x.size(2))
        x += residual
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Dictionary to hold the models, scalers, sensor configurations, and class mappings for all users
user_artifacts = {}

# Load the most recent model, scaler, and sensor configurations for each user
def load_latest_user_artifacts():
    conn = get_db_connection()
    cursor = conn.cursor()

    query = """
    SELECT DISTINCT ON (userid) userid, model, class_mapping, scaler, sensors_used
    FROM public.training_job_artifacts
    ORDER BY userid, job_id DESC;
    """
    cursor.execute(query)
    results = cursor.fetchall()
    conn.close()

    if not results:
        raise Exception("No models found in the database.")

    for userid, model_blob, class_mapping_json, scaler_blob, sensors_used_json in results:
        # Deserialize the model state_dict
        state_dict = pickle.loads(model_blob)
        
        # Remove '_orig_mod.' prefix from keys if present
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace("_orig_mod.", "")
            new_state_dict[new_key] = value

        num_classes = len(class_mapping_json)  # Since class_mapping_json is already a dict
        model = EnhancedAudioCNN(num_classes=num_classes)
        model.load_state_dict(new_state_dict)  # Load the adjusted state_dict
        model.eval()

        # Load the scaler
        scaler = pickle.loads(scaler_blob)

        # Load sensors used (assuming this is a JSON string)
        sensors_used = json.loads(sensors_used_json) if isinstance(sensors_used_json, str) else sensors_used_json

        # Load class mapping
        class_mapping = json.loads(class_mapping_json) if isinstance(class_mapping_json, str) else class_mapping_json

        # Store artifacts in the dictionary
        user_artifacts[userid] = {
            "model": model,
            "scaler": scaler,
            "sensors_used": sensors_used,
            "class_mapping": class_mapping
        }

# Preprocessing function
def preprocess(data, scaler, sensors_used):
    try:
        # Determine which sensors to use
        columns_to_use = [col for col, used in sensors_used.items() if used]
        
        # Create DataFrame and preprocess the data
        df = pd.DataFrame(data)
        df = df.loc[:, 0:len(columns_to_use) - 1]  # Adjust the range to match the length of columns_to_use
        df.columns = columns_to_use
        df = scaler.transform(df.values)
        
        sequences = []
        for row in range(32, df.shape[0]):
            sequence = df[row-32:row].T.flatten()
            sequences.append(sequence)

        sequences = np.array(sequences)
        return torch.tensor(sequences, dtype=torch.float32)
    
    except ValueError as ve:
        print(f"ValueError occurred: {ve}")
        raise ve
    except IndexError as ie:
        print(f"IndexError occurred: {ie}")
        raise ie
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        raise e

# Prediction endpoint
@app.post("/predict")
async def predict(input_data: InputData):
    try:
        # Retrieve the model, scaler, and sensors used for the given user ID
        if input_data.userid not in user_artifacts:
            raise HTTPException(status_code=404, detail="No model found for the given user.")

        model_info = user_artifacts[input_data.userid]
        model = model_info["model"]
        scaler = model_info["scaler"]
        sensors_used = model_info["sensors_used"]
        class_mapping = model_info["class_mapping"]
        
        # Preprocess the input data
        data = input_data.data
        inputs = preprocess(data, scaler, sensors_used)
        inputs = inputs.unsqueeze(1)
        
        # Make predictions
        with torch.no_grad():
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            # Convert predicted indices to gesture descriptions
            predicted_gestures = [class_mapping[str(idx)] for idx in predicted.tolist()]
            return {"predictions": predicted_gestures}
    
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.on_event("startup")
async def startup_event():
    # Load only the most recent artifacts for each user at startup
    load_latest_user_artifacts()

def get_db_connection():
    return psycopg2.connect(
        host="localhost",
        database="sensordb",
        user="XXXXXX", # Your username here
        password="XXXXXX"  # Your password here
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


---
./ml_model/nat_inference.py
---
import asyncio
import requests
import psycopg2
from nats.aio.client import Client as NATS
from datetime import datetime

NATS_SERVER = "nats://127.0.0.1:4222"
NATS_USER = "XXXXXXX" # Your username here
NATS_PASSWORD = "XXXXXXX" # Your password here
NATS_TOPIC = "sensor.data"
API_URL = "http://127.0.0.1:8000/predict"
BATCH_SIZE = 64

# PostgreSQL connection parameters
POSTGRES_HOST = "localhost"
POSTGRES_DB = "sensordb"
POSTGRES_USER = "XXXXXXX"
POSTGRES_PASSWORD = "XXXXXXXX"  # Your password here

# List to store received data
data_list = []

# Store the API response in the PostgreSQL database
async def store_api_response(userid, response):
    try:
        conn = psycopg2.connect(
            host=POSTGRES_HOST,
            database=POSTGRES_DB,
            user=POSTGRES_USER,
            password=POSTGRES_PASSWORD
        )
        cursor = conn.cursor()
        timestamp = datetime.now().isoformat()
        cursor.execute('''
            INSERT INTO api_responses (userid, response, timestamp)
            VALUES (%s, %s, %s)
        ''', (userid, response, timestamp))
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:
        print(f"Failed to store API response in PostgreSQL. Error: {e}")

# Function to process the sensor data and make API call
async def process_sensor_data():
    try:
        # Extract the user ID from the first entry (assuming it's the same for all entries in the batch)
        userid = data_list[0]["userid"]

        # Sort data_list by millis
        sorted_data = sorted(data_list, key=lambda x: x["millis"])

        # Extract the first 4 sensor values for each entry
        sorted_sensor_values = [entry["sensor_values"][:4] for entry in sorted_data]

        # Prepare the payload for the API call
        payload = {
            "userid": userid,
            "data": sorted_sensor_values
        }

        try:
            # Send the POST request to the /predict endpoint
            response = requests.post(API_URL, json=payload)
            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)

            # Print the response
            print(datetime.now().isoformat(), response.json())

            # Store the response in the PostgreSQL database
            await store_api_response(userid, str(response.json()))

        except requests.exceptions.RequestException as e:
            # Handle any exceptions that occur during the request
            print(f"Failed to send request to the API. Error: {e}")
            await store_api_response(userid, f"Request failed: {e}")

    except Exception as e:
        print(f"Failed to process sensor data. Error: {e}")

# Callback function to handle messages from NATS
async def message_handler(msg):
    try:
        data_str = msg.data.decode()
        data = eval(data_str)  # Use json.loads for a more secure conversion if the data is JSON formatted
        data_list.append(data)

        # Process data if we have accumulated enough messages
        if len(data_list) >= BATCH_SIZE:
            await process_sensor_data()
            data_list.clear()
    except Exception as e:
        print(f"Failed to handle message. Error: {e}")

# Main function to subscribe to NATS topic and process data
async def main():
    nc = NATS()

    # Connect to the NATS server with authentication
    await nc.connect(
        servers=[NATS_SERVER],
        user=NATS_USER,
        password=NATS_PASSWORD
    )

    # Subscribe to the NATS topic
    await nc.subscribe(NATS_TOPIC, cb=message_handler)

    # Keep the subscriber running
    try:
        while True:
            await asyncio.sleep(0.2)  # Sleep for a second and keep checking
    finally:
        await nc.drain()

if __name__ == "__main__":
    asyncio.run(main())


---
./streamlit_apps/Main.py
---
import streamlit as st

st.title("Personalized Gesture Recognition System")

st.write("Use the sidebar to navigate to the User or Gesture management pages.")


---
./streamlit_apps/pages/1_User_Management.py
---
import streamlit as st
import psycopg2
import pandas as pd

# Set up a connection to the PostgreSQL database
def init_connection():
    return psycopg2.connect(
        host="localhost",       # Adjust as necessary
        database="sensordb",
        user="XXXXXXX",
        password="XXXXXXXX"
    )

# Initialize connection
conn = init_connection()
cursor = conn.cursor()

# Function to create a new user
def create_user(first_name, last_name, email, description):
    query = """INSERT INTO users (first_name, last_name, email, personal_description) 
               VALUES (%s, %s, %s, %s);"""
    cursor.execute(query, (first_name, last_name, email, description))
    conn.commit()

# Function to fetch all users
def fetch_users():
    cursor.execute("SELECT * FROM users;")
    return cursor.fetchall()

# Function to update a user
def update_user(userid, first_name, last_name, email, description):
    query = """UPDATE users SET first_name = %s, last_name = %s, email = %s, personal_description = %s 
               WHERE userid = %s;"""
    cursor.execute(query, (first_name, last_name, email, description, userid))
    conn.commit()

# Function to delete a user
def delete_user(userid):
    cursor.execute("DELETE FROM users WHERE userid = %s;", (userid,))
    conn.commit()

# Streamlit Interface
st.title("User Management")

# Create User Form
with st.form("create_user"):
    st.subheader("Create a New User")
    first_name = st.text_input("First Name")
    last_name = st.text_input("Last Name")
    email = st.text_input("Email")
    description = st.text_area("Personal Description")
    submitted = st.form_submit_button("Create User")
    
    if submitted:
        create_user(first_name, last_name, email, description)
        st.success(f"User {first_name} {last_name} created successfully!")

# List all users
st.subheader("All Users")
users = fetch_users()
users_df = pd.DataFrame(users, columns=["UserID", "First Name", "Last Name", "Email", "Description"])
st.dataframe(users_df)

# Update/Delete User
st.subheader("Update or Delete a User")
user_id_to_modify = st.selectbox("Select a User to Modify", users_df['UserID'])

if user_id_to_modify:
    # Select user information
    selected_user = users_df[users_df['UserID'] == user_id_to_modify].iloc[0]
    
    # Update form
    with st.form("update_user"):
        first_name = st.text_input("First Name", selected_user['First Name'])
        last_name = st.text_input("Last Name", selected_user['Last Name'])
        email = st.text_input("Email", selected_user['Email'])
        description = st.text_area("Personal Description", selected_user['Description'])
        update_submitted = st.form_submit_button("Update User")
        
        if update_submitted:
            update_user(user_id_to_modify, first_name, last_name, email, description)
            st.success(f"User {first_name} {last_name} updated successfully!")
    
    # Delete user button
    if st.button("Delete User"):
        delete_user(user_id_to_modify)
        st.success(f"User {user_id_to_modify} deleted successfully!")
        


---
./streamlit_apps/pages/2_Gesture_Management.py
---
import streamlit as st
import psycopg2
import pandas as pd

# Set up a connection to the PostgreSQL database
def init_connection():
    return psycopg2.connect(
        host="localhost",       # Adjust as necessary
        database="sensordb",
        user="kaavya",
        password="sEMG1234"
    )

# Initialize connection
conn = init_connection()
cursor = conn.cursor()

def delete_gesture(gestureid):
    query = "DELETE FROM user_gestures WHERE gestureid = %s;"
    cursor.execute(query, (gestureid,))
    conn.commit()

# Function to fetch all users
def fetch_users():
    cursor.execute("SELECT * FROM users;")
    return cursor.fetchall()

# Function to fetch user gestures for a specific user
def fetch_gestures(userid):
    cursor.execute("SELECT * FROM user_gestures WHERE userid = %s;", (userid,))
    return cursor.fetchall()

# Function to create a gesture for a user
def create_gesture(userid, description, sensors):
    query = """INSERT INTO user_gestures (userid, gesture_description, sensor_a0_used, sensor_a1_used, 
               sensor_a2_used, sensor_a3_used, sensor_a4_used, sensor_a0_purpose, sensor_a1_purpose, 
               sensor_a2_purpose, sensor_a3_purpose, sensor_a4_purpose)
               VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);"""
    cursor.execute(query, (userid, description, sensors['sensor_a0_used'], sensors['sensor_a1_used'], 
                           sensors['sensor_a2_used'], sensors['sensor_a3_used'], sensors['sensor_a4_used'], 
                           sensors['sensor_a0_purpose'], sensors['sensor_a1_purpose'], sensors['sensor_a2_purpose'], 
                           sensors['sensor_a3_purpose'], sensors['sensor_a4_purpose']))
    conn.commit()

# Streamlit Interface
st.title("Gesture Management")

# Fetch users and show drop-down with First Name + Last Name
users = fetch_users()
users_df = pd.DataFrame(users, columns=["UserID", "First Name", "Last Name", "Email", "Description"])
users_df["Full Name"] = users_df["First Name"] + " " + users_df["Last Name"]

# Select user by Full Name
selected_user = st.selectbox("Select User for Gestures", users_df["Full Name"])

# Find the UserID of the selected user
selected_user_row = users_df[users_df["Full Name"] == selected_user]
selected_user_id = int(selected_user_row["UserID"].values[0])

if selected_user_id:
    # Create gesture with sensor inputs
    with st.form("create_gesture"):
        st.subheader(f"Create Gesture for {selected_user}")
        
        # Gesture description
        gesture_description = st.text_input("Gesture Description")
  
        
        # Create checkboxes and corresponding text inputs
        col1, col2 = st.columns(2)
        sensor_a0_used = col1.checkbox("Use Sensor A0")
        sensor_a0_purpose = col2.text_input("Sensor A0 Description")
        col1, col2 = st.columns(2)
        sensor_a1_used = col1.checkbox("Use Sensor A1")
        sensor_a1_purpose = col2.text_input("Sensor A1 Description")
        col1, col2 = st.columns(2)
        sensor_a2_used = col1.checkbox("Use Sensor A2")
        sensor_a2_purpose = col2.text_input("Sensor A2 Description")
        col1, col2 = st.columns(2)
        sensor_a3_used = col1.checkbox("Use Sensor A3")
        sensor_a3_purpose = col2.text_input("Sensor A3 Description")
        col1, col2 = st.columns(2)
        sensor_a4_used = col1.checkbox("Use Sensor A4")
        sensor_a4_purpose = col2.text_input("Sensor A4 Description")

        # Collect all the sensor data
        sensors = {
            'sensor_a0_used': sensor_a0_used,
            'sensor_a1_used': sensor_a1_used,
            'sensor_a2_used': sensor_a2_used,
            'sensor_a3_used': sensor_a3_used,
            'sensor_a4_used': sensor_a4_used,
            'sensor_a0_purpose': sensor_a0_purpose,
            'sensor_a1_purpose': sensor_a1_purpose,
            'sensor_a2_purpose': sensor_a2_purpose,
            'sensor_a3_purpose': sensor_a3_purpose,
            'sensor_a4_purpose': sensor_a4_purpose
        }

        # Submit button
        create_gesture_submitted = st.form_submit_button("Create Gesture")
        
        if create_gesture_submitted:
            create_gesture(selected_user_id, gesture_description, sensors)
            st.success(f"Gesture created successfully for {selected_user}!")

# List all user gestures
    st.markdown("---")
    st.subheader(f"Gestures for {selected_user}")
    gestures = fetch_gestures(selected_user_id)
    gestures_df = pd.DataFrame(gestures, columns=["GestureID", "UserID", "Gesture Description", 
                                                  "Sensor A0 Used", "Sensor A1 Used", 
                                                  "Sensor A2 Used", "Sensor A3 Used", 
                                                  "Sensor A4 Used", "Sensor A0 Purpose", 
                                                  "Sensor A1 Purpose", "Sensor A2 Purpose", 
                                                  "Sensor A3 Purpose", "Sensor A4 Purpose"])
    
    # Remove UserID from the display
    gestures_df = gestures_df.drop(columns=["UserID"])

    # Add delete buttons next to each gesture
    
    for i, row in gestures_df.iterrows():
        col1, col2 = st.columns([4, 1])
        col1.write(row)
        delete_button = col2.button("Delete", key=row["GestureID"])
        if delete_button:
            delete_gesture(row["GestureID"])
            st.success(f"Gesture {row['Gesture Description']} deleted successfully!")
            st.rerun()

---
./streamlit_apps/pages/3_Training_Data_Capture.py
---
import streamlit as st
from streamlit_webrtc import webrtc_streamer
import av
import cv2
import time
from datetime import datetime, timedelta
import psycopg2
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

# Initialize database connection
def init_connection():
    return psycopg2.connect(
        host="localhost",       # Adjust as necessary
        database="sensordb",
        user="kaavya",
        password="sEMG1234"
    )

# Initialize connection
conn = init_connection()

# Function to insert the timing plan into the database with start and end times
def insert_timing_plan_async(user_id, gesture_id, timestamp, sample_number, status, start_time, end_time):
    try:
        cursor = conn.cursor()
        query = """
        INSERT INTO public.user_gesture_trainingmetadata (userid, gestureid, "timestamp", sample_number, status, start_time, end_time) 
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        """
        cursor.execute(query, (user_id, gesture_id, timestamp, sample_number, status, start_time, end_time))
        conn.commit()
        cursor.close()
    except Exception as e:
        print(f"Error inserting timing plan: {e}")

# Function to insert video frames into the database asynchronously
def insert_frame_async(user_id, frame_bytes):
    try:
        cursor = conn.cursor()
        query = """
        INSERT INTO public.user_video (userid, video_frame, "timestamp") 
        VALUES (%s, %s, CURRENT_TIMESTAMP)
        """
        cursor.execute(query, (user_id, frame_bytes))
        conn.commit()
        cursor.close()
    except Exception as e:
        print(f"Error inserting frame: {e}")

# Convert frame to byte array
def frame_to_bytes(frame):
    is_success, buffer = cv2.imencode(".jpg", frame)
    if not is_success:
        raise Exception("Failed to convert frame to bytes")
    return buffer.tobytes()

# Thread pool for asynchronous insertion
executor = ThreadPoolExecutor()

# Function to fetch all users
def fetch_users():
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users;")
    return cursor.fetchall()

# Function to fetch user gestures for a specific user
def fetch_gestures(userid):
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM user_gestures WHERE userid = %s;", (userid,))
    return cursor.fetchall()

# Timing plan generation function and insert into DB with start and end time
def create_timing_plan_with_active_pause(gesture_ids, samples_per_gesture, seconds_for_gesture_capture, seconds_for_rest, user_id):
    plan_data = []
    current_time = datetime.now()

    for gesture_id in gesture_ids:
        for sample_num in range(1, samples_per_gesture + 1):
            start_time = current_time
            # Rest entry
            plan_data.append({
                'timestamp': current_time,
                'gesture': int(gesture_id),
                'sample number': int(sample_num),
                'status': 'REST PERIOD'
            })
            executor.submit(insert_timing_plan_async, user_id, gesture_id, current_time, sample_num, 'REST PERIOD', start_time, current_time + timedelta(seconds=seconds_for_rest))
            current_time += timedelta(seconds=seconds_for_rest)

            # Active entry
            start_time = current_time
            plan_data.append({
                'timestamp': current_time,
                'gesture': int(gesture_id),
                'sample number': int(sample_num),
                'status': 'MAKE GESTURE'
            })
            executor.submit(insert_timing_plan_async, user_id, gesture_id, current_time, sample_num, 'MAKE GESTURE', start_time, current_time + timedelta(seconds=seconds_for_gesture_capture))
            current_time += timedelta(seconds=seconds_for_gesture_capture)

        # Completion entry for gesture
        plan_data.append({
            'timestamp': current_time,
            'gesture': int(gesture_id),
            'sample number': int(sample_num),
            'status': 'SAMPLE COMPLETE'
        })
        executor.submit(insert_timing_plan_async, user_id, gesture_id, current_time, sample_num, 'SAMPLE COMPLETE', start_time, current_time)

    return pd.DataFrame(plan_data)

# Callback function to overlay the timing plan status on the video frame and save frame to DB
def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
    cv2.putText(img, current_timestamp, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)

    try:
        if 'timing_plan' not in st.session_state:
            cv2.putText(img, "NO TIMING PLAN", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)
            st.session_state.timing_plan = create_timing_plan_with_active_pause(
                selected_gestures, samples_per_gesture, seconds_per_gesture_capture, seconds_for_rest, selected_user_id)
        else:
            timing_plan = st.session_state.timing_plan
            current_row = timing_plan.loc[(timing_plan['timestamp'] <= current_timestamp)].tail(1)
            if len(current_row) > 0:
                status = current_row['status'].values[0]
                if status != 'SAMPLE COMPLETE':
                    gesture = current_row['gesture'].values[0]
                    ts = pd.to_datetime(current_row['timestamp'].values[0])
                    sample_num = current_row['sample number'].values[0]
                    wseconds = seconds_per_gesture_capture if status == 'MAKE GESTURE' else seconds_for_rest
                    wait_time = ((ts + timedelta(seconds=wseconds)) - datetime.now()).seconds
                    cv2.putText(img, f"GESTURE ID: {gesture}", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
                    cv2.putText(img, f"SAMPLE    : {sample_num}", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
                    cv2.putText(img, f"ACTION    : {status}", (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)
                    cv2.putText(img, f"WAIT TIME : {wait_time}", (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)

                    # Convert frame to bytes and insert it into the DB asynchronously
                    frame_bytes = frame_to_bytes(img)
                    executor.submit(insert_frame_async, selected_user_id, frame_bytes)

                else:
                    cv2.putText(img, "SAMPLE COLLECTION COMPLETED", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
    except:
        cv2.putText(img, "Error checking for timing plan", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

    return av.VideoFrame.from_ndarray(img, format="bgr24")


# Streamlit UI for managing gesture data capture
st.title("Personalized Gesture Recognition System: Training Data Capture")

# Fetch users from the database
users = fetch_users()
users_df = [(user[0], f"{user[1]} {user[2]}") for user in users]  # Assuming user[1] = first name, user[2] = last name

# Separate the names and user IDs
user_ids = [user[0] for user in users]
user_names = [f"{user[1]} {user[2]}" for user in users]

# Dropdown to select user (Only show the name in the dropdown)
st.subheader("Select User")
selected_user_name = st.selectbox("User", user_names)

# Find the corresponding user ID
selected_user_id = user_ids[user_names.index(selected_user_name)]

# Number of samples per gesture and seconds per gesture inputs
col1, col2 = st.columns(2)
samples_per_gesture = col1.number_input("Samples per Gesture", min_value=1, value=5)
seconds_per_gesture_capture = col2.number_input("Seconds per Gesture", min_value=0.5, value=2.5, step=0.1)
seconds_for_rest = col2.number_input("Seconds for Rest", min_value=0.5, value=5.0, step=0.5)

# Display user gestures for the selected user
user_gestures = fetch_gestures(selected_user_id)
gestures_df = [(gesture[0], gesture[2]) for gesture in user_gestures]  # Assuming gesture[2] = gesture description

# Start data collection button
st.session_state.gesture_ids = [g[0] for g in gestures_df]
st.session_state.samples_per_gesture = samples_per_gesture
st.session_state.seconds_per_gesture_capture = seconds_per_gesture_capture
st.session_state.seconds_for_rest = seconds_for_rest
st.write(f"Collect Training Data for the following gestures: {','.join([g[1] for g in gestures_df])}")
selected_gestures = st.multiselect(f'Gestures Enabled for Training Data Collection for {selected_user_name}:',
                                   st.session_state.gesture_ids, default=st.session_state.gesture_ids)

# Display the video stream
st.subheader("Webcam Video Stream (for Gesture Recording)")
webrtc_streamer(key="training_stream",
                rtc_configuration={  # Add this config
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
                        },
                 video_frame_callback=video_frame_callback)

---
./streamlit_apps/pages/4_Kickoff_Training.py
---
import streamlit as st
import psycopg2
import pandas as pd
import plotly.express as px
import cv2
import numpy as np
import tempfile
import subprocess
# Initialize database connection
def init_connection():
    return psycopg2.connect(
        host="localhost",  # Adjust as necessary
        database="sensordb",
        user="kaavya",
        password="sEMG1234"  # Your password here
    )

# Initialize connection
conn = init_connection()

# Function to fetch users
def fetch_users():
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users;")
    return cursor.fetchall()

# Function to fetch training metadata for a user
def fetch_training_metadata_for_user(user_id):
    cursor = conn.cursor()
    query = """
    SELECT * FROM user_gesture_trainingmetadata WHERE userid = %s;
    """
    cursor.execute(query, (user_id,))
    rows = cursor.fetchall()
    return rows

# Function to fetch raw sensor data based on userid, start_time, and end_time
def fetch_sensor_data(user_id, start_time, end_time):
    cursor = conn.cursor()
    
    user_id = int(user_id)
    start_time = pd.Timestamp(start_time).to_pydatetime()
    end_time = pd.Timestamp(end_time).to_pydatetime()
    
    query = """
    SELECT * FROM user_sensor 
    WHERE userid = %s AND ts BETWEEN %s AND %s;
    """
    cursor.execute(query, (user_id, start_time, end_time))
    rows = cursor.fetchall()
    return rows


# Function to run the training job in the background using subprocess
def submit_job_in_background(job_id):
    try:
        # Construct the command
        command = ["python3", "../ml_model/TrainMLJob.py", str(job_id)]
        
        # Run the command using subprocess in the background
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)
        
        st.success(f"Training job {job_id} processing has begun")

    except Exception as e:
        st.error(f"An error occurred while submitting the job: {e}")

# Function to fetch video frames for the selected session
def fetch_video_frames(user_id, start_time, end_time):
    cursor = conn.cursor()
    
    query = """
    SELECT video_frame, "timestamp" FROM user_video 
    WHERE userid = %s AND "timestamp" BETWEEN %s AND %s;
    """
    cursor.execute(query, (user_id, start_time, end_time))
    rows = cursor.fetchall()
    return rows

# Function to convert rows to a pandas DataFrame
def convert_to_dataframe(rows, columns):
    return pd.DataFrame(rows, columns=columns)

def save_frames_to_video(frames, frame_size, fps=30):
    import tempfile
    import cv2

    temp_video_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
    video_path = temp_video_file.name

    # Initialize the VideoWriter with H.264 codec
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # H.264 codec
    out = cv2.VideoWriter(video_path, fourcc, fps, frame_size)

    for frame_bgr in frames:
        out.write(frame_bgr)  # No need to convert to RGB when writing

    out.release()
    return video_path


# Function to kick off training process
def kick_off_training(selected_data):
    conn = init_connection()
    cursor = conn.cursor()
    
    try:
        # Convert numpy types to native Python types
        user_id = int(selected_data.iloc[0]['userid'])  # Assuming all rows have the same user_id
        num_samples = int(len(selected_data))
        
        insert_job_query = """
        INSERT INTO training_job_schedule (userid, job_status, num_samples)
        VALUES (%s, %s, %s) RETURNING job_id;
        """
        cursor.execute(insert_job_query, (user_id, 'scheduled', num_samples))
        job_id = cursor.fetchone()[0]
        
        # Insert associated training metadata into job_training_metadata
        insert_metadata_query = """
        INSERT INTO job_training_metadata (job_id, training_metadata_id)
        VALUES (%s, %s);
        """
        for _, row in selected_data.iterrows():
            training_metadata_id = int(row['training_metadata_id'])
            cursor.execute(insert_metadata_query, (job_id, training_metadata_id))
        
        # Commit the transaction
        conn.commit()
        st.success(f"Training job {job_id} has been scheduled successfully!")
        submit_job_in_background(job_id)
        st.success(f"Training job {job_id} processing has begun ")




    except Exception as e:
        conn.rollback()
        st.error(f"An error occurred while scheduling the training job: {str(e)}")
    finally:
        cursor.close()
        conn.close()



# Streamlit UI
st.title("Review and Kick Off Gesture Training")

# Fetch all users
users = fetch_users()

# Create a dropdown to select a user
user_options = [(f"{user[1]} {user[2]}", user[0]) for user in users]  # (user_full_name, user_id)
user_selected = st.selectbox("Select User", user_options, format_func=lambda x: x[0])

if user_selected:
    user_id = user_selected[1]

    # Fetch training metadata for the selected user
    training_data = fetch_training_metadata_for_user(user_id)
    
    if training_data:
        training_columns = ["training_metadata_id", "userid", "gestureid", "timestamp", "sample_number", "status", "start_time", "end_time"]
        df = convert_to_dataframe(training_data, training_columns)
        st.subheader("Training Metadata for User")
        st.dataframe(df)

        if 'selected_rows' not in st.session_state:
            st.session_state.selected_rows = [False] * len(df)

        col1, col2 = st.columns(2)
        if col1.button("Select All"):
            st.session_state.selected_rows = [True] * len(df)
        if col2.button("Unselect All"):
            st.session_state.selected_rows = [False] * len(df)

        st.subheader("Select Training Sessions")
        cols = st.columns(5)  # Create 5 columns
        selected_rows = []
        for idx, row in df.iterrows():
            col = cols[idx % 5]
            if col.checkbox(f"Session {row['training_metadata_id']}", key=idx, value=st.session_state.selected_rows[idx]):
                selected_rows.append(idx)
                st.session_state.selected_rows[idx] = True
            else:
                st.session_state.selected_rows[idx] = False

        if selected_rows:
            selected_data = df.loc[selected_rows]
            st.write("Selected Data for Training")
            st.dataframe(selected_data)

            # Dropdown to select one session for displaying sensor data
            st.subheader("Select a Session to View Raw Sensor Data")
            session_selected = st.selectbox("Select Session", selected_data['training_metadata_id'].unique())

            if session_selected:
                session_data = selected_data[selected_data['training_metadata_id'] == session_selected].iloc[0]
                start_time = session_data['start_time']
                end_time = session_data['end_time']

                # Fetch sensor data for the selected session
                sensor_data = fetch_sensor_data(session_data['userid'], start_time, end_time)
                if sensor_data:
                    sensor_columns = ["id", "userid", "millis", "sensor_a0", "sensor_a1", "sensor_a2", "sensor_a3", "sensor_a4", "timestamp"]
                    sensor_df = convert_to_dataframe(sensor_data, sensor_columns)
                    
                    st.subheader(f"Raw Sensor Data for Session {session_selected}")
                    fig = px.line(sensor_df, x='timestamp', y=['sensor_a0', 'sensor_a1', 'sensor_a2', 'sensor_a3', 'sensor_a4'],
                                  labels={'timestamp': 'Time', 'value': 'Sensor Values'},
                                  title=f"Sensor Data for Session {session_selected}")
                    st.plotly_chart(fig)
                else:
                    st.write("No sensor data found for this session.")

                # Fetch and display video frames
                st.subheader(f"Video for Session {session_selected}")
                video_frames = fetch_video_frames(int(session_data['userid']), start_time, end_time)
                if video_frames:
                    st.write(f"Found {len(video_frames)} video frames.")
                    frame_size = (640, 480)  # You can modify this based on your video dimensions
                    frames = []
                    for frame, frame_timestamp in video_frames:
                        img_np = np.frombuffer(frame, dtype=np.uint8)
                        img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
                        #img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        frames.append(img)
                    
                    # Save frames to a video file
                    video_file_path = save_frames_to_video(frames, frame_size)
                    
                    # Display video in Streamlit
                    st.video(video_file_path)
                else:
                    st.write("No video frames found for this session.")
            
            if st.button("Kick Off Training"):
                kick_off_training(selected_data)
    else:
        st.write("No training data found for this user.")


---
./streamlit_apps/pages/5_Training_Jobs_Status.py
---
import streamlit as st
import psycopg2
import pandas as pd
import plotly.express as px

# Initialize database connection
def init_connection():
    return psycopg2.connect(
        host="localhost",
        database="sensordb",
        user="kaavya",
        password="sEMG1234"  # Your password here
    )

# Function to fetch users
def fetch_users(conn):
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users;")
    return cursor.fetchall()

# Function to fetch training job status for a user, ordered by most recent first
def fetch_training_jobs_for_user(conn, user_id):
    cursor = conn.cursor()
    query = """
    SELECT job_id, job_status, num_samples, actual_start_time, actual_end_time, error_message, log_messages
    FROM training_job_schedule 
    WHERE userid = %s
    ORDER BY job_id DESC;  -- Most recent job first
    """
    cursor.execute(query, (user_id,))
    rows = cursor.fetchall()
    return rows

# Streamlit UI
st.title("Training Job Status")

# Initialize connection
conn = init_connection()



# Fetch all users
users = fetch_users(conn)

# Create a dropdown to select a user
user_options = [(f"{user[1]} {user[2]}", user[0]) for user in users]  # (user_full_name, user_id)
user_selected = st.selectbox("Select User", user_options, format_func=lambda x: x[0])

if user_selected:
    user_id = user_selected[1]

    # Fetch training jobs for the selected user
    training_jobs = fetch_training_jobs_for_user(conn, user_id)
    
    if training_jobs:
        job_columns = ["Job ID", "Status", "Number of Samples", "Start Time", "End Time", "Error Message", "Log Messages"]
        df = pd.DataFrame(training_jobs, columns=job_columns)
        
        st.subheader("Training Jobs for User")
        st.dataframe(df)  # Display the DataFrame with the most recent job first
        
        # Show detailed view of a selected job
        st.subheader("Select a Job to View Details")
        job_selected = st.selectbox("Select Job ID", df['Job ID'].unique())
        
        if job_selected:
            # Refresh button
            if st.button("Refresh Job Status"):
                st.rerun()
            selected_job = df[df['Job ID'] == job_selected].iloc[0]
            st.write("### Job Details:")
            cols=st.columns(5)
            cols[0].write(f"**Job ID:** {selected_job['Job ID']}")
            cols[1].write(f"**Status:** {selected_job['Status']}")
            cols[2].write(f"**Number of Samples:** {selected_job['Number of Samples']}")
            cols[3].write(f"**Start Time:** {selected_job['Start Time']}")
            cols[4].write(f"**End Time:** {selected_job['End Time']}")
            st.write(f"**Error Message:** {selected_job['Error Message']}")
            st.write("### Log Messages:")
            st.text_area("", selected_job['Log Messages'], height=200)
    else:
        st.write("No training jobs found for this user.")

# Close the connection
conn.close()


---
./streamlit_apps/pages/6_Realtime_Gesture_Recognition.py
---
import asyncio
import psycopg2
import streamlit as st
import plotly.graph_objects as go
import pandas as pd
from datetime import datetime, timedelta
from scipy.interpolate import interp1d
import numpy as np
from collections import Counter
import re

# Initialize PostgreSQL connection
def init_connection():
    return psycopg2.connect(
        host="localhost",
        database="sensordb",
        user="kaavya",
        password="sEMG1234"  # Replace with your PostgreSQL password
    )

conn = init_connection()

# Fetch users from the database
def fetch_users():
    cursor = conn.cursor()
    cursor.execute("SELECT userid, first_name, last_name FROM users;")
    users = cursor.fetchall()
    cursor.close()
    return users

# Fetch sensor data for a specific user within a time window
async def fetch_sensor_data(conn, user_id, time_window):
    cursor = conn.cursor()
    time_threshold = (datetime.now() - timedelta(seconds=time_window / 1000)).isoformat()
    cursor.execute('''
        SELECT millis, sensor_a0, sensor_a1, sensor_a2, sensor_a3, sensor_a4, ts 
        FROM user_sensor 
        WHERE userid = %s AND ts > %s 
        ORDER BY millis DESC
    ''', (user_id, time_threshold))
    rows = cursor.fetchall()
    cursor.close()
    return rows

# Fetch gesture data for a specific user within a time window
async def fetch_gesture_data(conn, user_id, time_window):
    cursor = conn.cursor()
    time_threshold = (datetime.now() - timedelta(seconds=time_window / 1000)).isoformat()
    cursor.execute('''
        SELECT response, timestamp 
        FROM api_responses 
        WHERE userid = %s AND timestamp > %s
        ORDER BY timestamp DESC
    ''', (user_id, time_threshold))
    rows = cursor.fetchall()
    cursor.close()
    return rows

# Interpolate sensor data for smooth plotting
def interpolate_data(df):
    new_millis = np.linspace(df['millis'].min(), df['millis'].max(), num=500)
    interpolated_df = pd.DataFrame({'millis': new_millis})
    
    for sensor in ['sensor_a0', 'sensor_a1', 'sensor_a2', 'sensor_a3', 'sensor_a4']:
        f = interp1d(df['millis'], df[sensor], kind='linear', fill_value='extrapolate')
        interpolated_df[sensor] = f(new_millis)
    
    return interpolated_df

def round_down(value, base=50):
    return base * (value // base)

def round_up(value, base=50):
    return base * ((value + base - 1) // base)

# Transform gesture data for plotting
def transform_gesture_data(gesture_data):
    def extract_predictions(s):
        match = re.search(r"\[(.*?)\]", s)
        if match:
            predictions_str = match.group(1)
            # Extract predictions as strings
            predictions = [x.strip().strip("'") for x in predictions_str.split(",")]
            return predictions
        return []

    parsed_data = []
    for row in gesture_data:
        predictions = extract_predictions(row[0])
        timestamp = pd.to_datetime(row[1])
        parsed_data.append((predictions, timestamp))

    transformed_data = []
    for predictions, timestamp in parsed_data:
        num_predictions = len(predictions)
        start_time = timestamp - timedelta(milliseconds=num_predictions)
        time_delta = timedelta(milliseconds=1)
        for i, prediction in enumerate(predictions):
            transformed_data.append((start_time + i * time_delta, prediction))

    df = pd.DataFrame(transformed_data, columns=['timestamp', 'prediction'])

    # Apply moving window manually to get the most frequent gesture
    window_size = 5
    if len(df) >= window_size:
        most_common_predictions = []
        for i in range(len(df)):
            if i < window_size:
                window_predictions = df['prediction'][:i+1]
            else:
                window_predictions = df['prediction'][i-window_size+1:i+1]
            most_common = Counter(window_predictions).most_common(1)[0][0]
            most_common_predictions.append(most_common)
        df['prediction'] = most_common_predictions

    return df[['timestamp', 'prediction']]

# Plot gesture data
def plot_gestures(gesture_df):
    fig = go.Figure()

    # Add scatter plot for gestures
    fig.add_trace(go.Scatter(
        x=gesture_df['timestamp'],
        y=gesture_df['prediction'],
        mode='markers+text',
        name='Gestures',
        text=gesture_df['prediction'],
        textposition='top center',
        textfont=dict(
            family="Arial",
            size=12,  # Adjusted for better readability
            color="black",
            weight='bold'
        ),
        marker=dict(size=10),
    ))

    # Rotate x-axis labels and adjust the step size
    fig.update_xaxes(tickangle=-45, nticks=20)  # Rotate labels by 45 degrees, reduce the number of ticks

    # Set the layout of the figure
    fig.update_layout(
        title='Classified Gestures Over Time',
        xaxis_title='Timestamp',
        yaxis_title='Prediction',
        template='plotly_white',
        yaxis=dict(range=[-1, 6]),  # Adjusted to include all gestures
        margin=dict(l=0, r=0, t=50, b=50),  # Adjust margins to provide more space
        height=400,  # Increased height for better spacing
    )

    return fig


# Plot sensor data
def plot_data(df, paused, sensors_to_display, show_interpolated, auto_y, manual_y_min, manual_y_max):
    if paused:
        return None

    if show_interpolated:
        df = interpolate_data(df)

    if auto_y:
        y_min = df[sensors_to_display].min().min()
        y_max = df[sensors_to_display].max().max()
        y_min_rounded = round_down(y_min, 50)
        y_max_rounded = round_up(y_max, 50)
    else:
        y_min_rounded = manual_y_min
        y_max_rounded = manual_y_max

    fig = go.Figure()
    for sensor in sensors_to_display:
        fig.add_trace(go.Scatter(x=df['millis'], y=df[sensor], mode='lines', name=sensor))

    fig.update_layout(
        title='Real-time Sensor Data and Classified Gestures',
        xaxis_title='Milliseconds since start',
        yaxis_title='Sensor Value',
        yaxis=dict(range=[y_min_rounded, y_max_rounded]),
        template='plotly_white'
    )
    
    return fig

# Main async function
async def main():
    st.set_page_config(layout="wide")  # Set layout to wide to use the entire screen width
    st.markdown('### Real-time Gesture Classification')
    users = fetch_users()
    user_options = {f"{user[1]} {user[2]}": user[0] for user in users}
    selected_user_name = st.selectbox("Select User", list(user_options.keys()), index=0)
    selected_user_id = user_options[selected_user_name]

    col1, col2 = st.columns(2)
    with col1:
        sensor_fig = st.empty()
    with col2:
        gesture_fig = st.empty()

    settings_expander = st.expander("Settings", expanded=True)
    with settings_expander:
        col1, col2, col3 = st.columns(3)
        with col1:
            paused = st.checkbox('Pause')
        with col2:
            show_interpolated = st.checkbox('Show Interpolated Data', value=False)
        with col3:
            time_window = st.slider('Time Window (ms)', min_value=10, max_value=10000, value=10000, step=10)

        auto_y = st.checkbox('Auto Y-Axis Scaling', value=True)
        if not auto_y:
            manual_y_min = st.number_input('Manual Y-Axis Min', value=0)
            manual_y_max = st.number_input('Manual Y-Axis Max', value=1000)
        else:
            manual_y_min = None
            manual_y_max = None

        st.subheader('Select Sensors to Display')
        sensor_cols = st.columns(5)
        sensors = ['sensor_a0', 'sensor_a1', 'sensor_a2', 'sensor_a3', 'sensor_a4']
        sensor_checkboxes = {sensor: sensor_cols[i].checkbox(sensor, value=True) for i, sensor in enumerate(sensors)}

        sensors_to_display = [sensor for sensor, checked in sensor_checkboxes.items() if checked]

    while True:
        sensor_data = await fetch_sensor_data(conn, selected_user_id, time_window)
        gesture_data = await fetch_gesture_data(conn, selected_user_id, time_window)
        
        sensors_to_display = [sensor for sensor, checked in sensor_checkboxes.items() if checked]
        df = pd.DataFrame(sensor_data, columns=['millis', 'sensor_a0', 'sensor_a1', 'sensor_a2', 'sensor_a3', 'sensor_a4', 'ts'])
        df = df.sort_values(by='millis')
        
        gesture_df = transform_gesture_data(gesture_data)

        sensor_fig.plotly_chart(plot_data(df, paused, sensors_to_display, show_interpolated, auto_y, manual_y_min, manual_y_max), use_container_width=True)
        gesture_fig.plotly_chart(plot_gestures(gesture_df), use_container_width=True)
        
        await asyncio.sleep(0.2)

if __name__ == '__main__':
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(main())


---
./streamlit_apps/pages/7_Sensor_Data_Review.py
---
import asyncio
import psycopg2
import streamlit as st
import plotly.graph_objects as go
import pandas as pd
from datetime import datetime, timedelta
from scipy.interpolate import interp1d
import numpy as np
from collections import Counter
import re

# Initialize PostgreSQL connection
def init_connection():
    return psycopg2.connect(
        host="localhost",
        database="sensordb",
        user="kaavya",
        password="sEMG1234"  # Replace with your PostgreSQL password
    )

conn = init_connection()

# Fetch users from the database
def fetch_users():
    cursor = conn.cursor()
    cursor.execute("SELECT userid, first_name, last_name FROM users;")
    users = cursor.fetchall()
    cursor.close()
    return users

# Fetch sensor data for a specific user within a time window
async def fetch_sensor_data(conn, user_id, time_window):
    cursor = conn.cursor()
    time_threshold = (datetime.now() - timedelta(seconds=time_window / 1000)).isoformat()
    cursor.execute(f'''
        SELECT millis, sensor_a0, sensor_a1, sensor_a2, sensor_a3, sensor_a4, ts 
        FROM user_sensor 
        WHERE userid = {user_id}
        ORDER BY millis DESC
    ''')
    rows = cursor.fetchall()
    cursor.close()
    return rows

# Fetch gesture data for a specific user within a time window
async def fetch_gesture_data(conn, user_id, time_window):
    cursor = conn.cursor()
    time_threshold = (datetime.now() - timedelta(seconds=time_window / 1000)).isoformat()
    cursor.execute(f'''
        SELECT response, timestamp 
        FROM api_responses 
        WHERE userid = {user_id}
        ORDER BY timestamp DESC
    ''')
    rows = cursor.fetchall()
    cursor.close()
    return rows

# Interpolate sensor data for smooth plotting
def interpolate_data(df):
    new_millis = np.linspace(df['millis'].min(), df['millis'].max(), num=500)
    interpolated_df = pd.DataFrame({'millis': new_millis})
    
    for sensor in ['sensor_a0', 'sensor_a1', 'sensor_a2', 'sensor_a3', 'sensor_a4']:
        f = interp1d(df['millis'], df[sensor], kind='linear', fill_value='extrapolate')
        interpolated_df[sensor] = f(new_millis)
    
    return interpolated_df

def round_down(value, base=50):
    return base * (value // base)

def round_up(value, base=50):
    return base * ((value + base - 1) // base)

# Transform gesture data for plotting
def transform_gesture_data(gesture_data):
    def extract_predictions(s):
        match = re.search(r"\[(.*?)\]", s)
        if match:
            predictions_str = match.group(1)
            # Extract predictions as strings
            predictions = [x.strip().strip("'") for x in predictions_str.split(",")]
            return predictions
        return []

    parsed_data = []
    for row in gesture_data:
        predictions = extract_predictions(row[0])
        timestamp = pd.to_datetime(row[1])
        parsed_data.append((predictions, timestamp))

    transformed_data = []
    for predictions, timestamp in parsed_data:
        num_predictions = len(predictions)
        start_time = timestamp - timedelta(milliseconds=num_predictions)
        time_delta = timedelta(milliseconds=1)
        for i, prediction in enumerate(predictions):
            transformed_data.append((start_time + i * time_delta, prediction))

    df = pd.DataFrame(transformed_data, columns=['timestamp', 'prediction'])

    # Apply moving window manually to get the most frequent gesture
    window_size = 5
    if len(df) >= window_size:
        most_common_predictions = []
        for i in range(len(df)):
            if i < window_size:
                window_predictions = df['prediction'][:i+1]
            else:
                window_predictions = df['prediction'][i-window_size+1:i+1]
            most_common = Counter(window_predictions).most_common(1)[0][0]
            most_common_predictions.append(most_common)
        df['prediction'] = most_common_predictions

    return df[['timestamp', 'prediction']]

# Plot gesture data
def plot_gestures(gesture_df):
    fig = go.Figure()

    # Add scatter plot for gestures
    fig.add_trace(go.Scatter(
        x=gesture_df['timestamp'],
        y=gesture_df['prediction'],
        mode='markers+text',
        name='Gestures',
        text=gesture_df['prediction'],
        textposition='top center',
        textfont=dict(
            family="Arial",
            size=12,  # Adjusted for better readability
            color="black",
            weight='bold'
        ),
        marker=dict(size=10),
    ))

    # Rotate x-axis labels and adjust the step size
    fig.update_xaxes(tickangle=-45, nticks=20)  # Rotate labels by 45 degrees, reduce the number of ticks

    # Set the layout of the figure
    fig.update_layout(
        title='Classified Gestures Over Time',
        xaxis_title='Timestamp',
        yaxis_title='Prediction',
        template='plotly_white',
        yaxis=dict(range=[-1, 6]),  # Adjusted to include all gestures
        margin=dict(l=0, r=0, t=50, b=50),  # Adjust margins to provide more space
        height=400,  # Increased height for better spacing
    )

    return fig


# Plot sensor data
def plot_data(df, paused, sensors_to_display, show_interpolated, auto_y, manual_y_min, manual_y_max):
    if paused:
        return None

    if show_interpolated:
        df = interpolate_data(df)

    if auto_y:
        y_min = df[sensors_to_display].min().min()
        y_max = df[sensors_to_display].max().max()
        y_min_rounded = round_down(y_min, 50)
        y_max_rounded = round_up(y_max, 50)
    else:
        y_min_rounded = manual_y_min
        y_max_rounded = manual_y_max

    fig = go.Figure()
    for sensor in sensors_to_display:
        fig.add_trace(go.Scatter(x=df['millis'], y=df[sensor], mode='lines', name=sensor))

    fig.update_layout(
        title='Real-time Sensor Data and Classified Gestures',
        xaxis_title='Milliseconds since start',
        yaxis_title='Sensor Value',
        yaxis=dict(range=[y_min_rounded, y_max_rounded]),
        template='plotly_white'
    )
    
    return fig

# Main async function
async def main():
    st.set_page_config(layout="wide")  # Set layout to wide to use the entire screen width
    st.markdown('### Real-time Gesture Classification')
    users = fetch_users()
    user_options = {f"{user[1]} {user[2]}": user[0] for user in users}
    selected_user_name = st.selectbox("Select User", list(user_options.keys()), index=0)
    selected_user_id = user_options[selected_user_name]

    col1, col2 = st.columns(2)
    with col1:
        sensor_fig = st.empty()
    with col2:
        gesture_fig = st.empty()

    settings_expander = st.expander("Settings", expanded=True)
    with settings_expander:
        col1, col2, col3 = st.columns(3)
        with col1:
            paused = st.checkbox('Pause')
        with col2:
            show_interpolated = st.checkbox('Show Interpolated Data', value=False)
        with col3:
            time_window = st.slider('Time Window (ms)', min_value=10, max_value=10000, value=10000, step=10)

        auto_y = st.checkbox('Auto Y-Axis Scaling', value=True)
        if not auto_y:
            manual_y_min = st.number_input('Manual Y-Axis Min', value=0)
            manual_y_max = st.number_input('Manual Y-Axis Max', value=1000)
        else:
            manual_y_min = None
            manual_y_max = None

        st.subheader('Select Sensors to Display')
        sensor_cols = st.columns(5)
        sensors = ['sensor_a0', 'sensor_a1', 'sensor_a2', 'sensor_a3', 'sensor_a4']
        sensor_checkboxes = {sensor: sensor_cols[i].checkbox(sensor, value=True) for i, sensor in enumerate(sensors)}

        sensors_to_display = [sensor for sensor, checked in sensor_checkboxes.items() if checked]

        sensor_data = await fetch_sensor_data(conn, selected_user_id, time_window)
        gesture_data = await fetch_gesture_data(conn, selected_user_id, time_window)
        
        sensors_to_display = [sensor for sensor, checked in sensor_checkboxes.items() if checked]
        df = pd.DataFrame(sensor_data, columns=['millis', 'sensor_a0', 'sensor_a1', 'sensor_a2', 'sensor_a3', 'sensor_a4', 'ts'])
        df = df.sort_values(by='millis')
        
        gesture_df = transform_gesture_data(gesture_data)

        sensor_fig.plotly_chart(plot_data(df, paused, sensors_to_display, show_interpolated, auto_y, manual_y_min, manual_y_max), use_container_width=True)
        gesture_fig.plotly_chart(plot_gestures(gesture_df), use_container_width=True)
        


if __name__ == '__main__':
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(main())


---
./testing/sampledataload.py
---
import psycopg2
import random
from datetime import datetime, timedelta

# Initialize database connection
def init_connection():
    return psycopg2.connect(
        host="localhost",  # Adjust to your setup
        database="sensordb",
        user="kaavya",
        password=""  # Your password here
    )

# Function to insert a single sensor data entry into the user_sensor table
def insert_sensor_data(cursor, userid, millis, sensors, timestamp):
    query = """
    INSERT INTO user_sensor (userid, millis, sensor_a0, sensor_a1, sensor_a2, sensor_a3, sensor_a4, ts) 
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """
    cursor.execute(query, (userid, millis, sensors[0], sensors[1], sensors[2], sensors[3], sensors[4], timestamp))

# Function to generate 50Hz sample data for given time intervals
def generate_and_insert_data(conn, userid, start_time, end_time):
    cursor = conn.cursor()
    
    # Calculate the duration in seconds and total samples (50 samples per second)
    total_seconds = (end_time - start_time).total_seconds()
    total_samples = int(total_seconds * 50)  # 50Hz data

    # Simulate sensor data for sensors A0 to A5 (6 sensors)
    for i in range(total_samples):
        # Generate random values for each sensor (you can replace this with real data)
        sensors = [random.uniform(0, 1) for _ in range(5)]
        
        # Calculate the timestamp and millis for each sample
        timestamp = start_time + timedelta(seconds=i / 50.0)  # 50Hz = 1/50th of a second
        millis = int(i * (1000 / 50))  # Millis per sample (for 50Hz, 20ms per sample)
        
        # Insert data into the database
        insert_sensor_data(cursor, userid, millis, sensors, timestamp)
    
    # Commit the transaction to save the data
    conn.commit()
    cursor.close()
input_string = """
2024-08-26 17:52:20-07:00	2024-08-26 17:52:25-07:00
2024-08-26 17:52:25-07:00	2024-08-26 17:52:27-07:00
2024-08-26 17:52:27-07:00	2024-08-26 17:52:32-07:00
2024-08-26 17:52:32-07:00	2024-08-26 17:52:35-07:00
2024-08-26 17:52:35-07:00	2024-08-26 17:52:40-07:00
2024-08-26 17:52:42-07:00	2024-08-26 17:52:47-07:00
2024-08-26 17:52:47-07:00	2024-08-26 17:52:50-07:00
2024-08-26 17:52:50-07:00	2024-08-26 17:52:55-07:00
2024-08-26 17:52:55-07:00	2024-08-26 17:52:57-07:00
2024-08-26 17:52:57-07:00	2024-08-26 17:53:02-07:00
2024-08-26 17:53:05-07:00	2024-08-26 17:53:10-07:00
2024-08-26 17:53:10-07:00	2024-08-26 17:53:12-07:00
2024-08-26 17:52:40-07:00	2024-08-26 17:52:42-07:00
2024-08-26 17:52:55-07:00	2024-08-26 17:52:57-07:00
2024-08-26 17:53:02-07:00	2024-08-26 17:53:05-07:00
2024-08-26 17:53:20-07:00	2024-08-26 17:53:25-07:00
2024-08-26 17:53:25-07:00	2024-08-26 17:53:27-07:00
2024-08-26 17:53:32-07:00	2024-08-26 17:53:35-07:00
2024-08-26 17:53:12-07:00	2024-08-26 17:53:17-07:00
2024-08-26 17:53:40-07:00	2024-08-26 17:53:42-07:00
2024-08-26 17:53:42-07:00	2024-08-26 17:53:47-07:00
2024-08-26 17:53:47-07:00	2024-08-26 17:53:50-07:00
2024-08-26 17:53:50-07:00	2024-08-26 17:53:55-07:00
2024-08-26 17:53:55-07:00	2024-08-26 17:53:57-07:00
2024-08-26 17:53:27-07:00	2024-08-26 17:53:32-07:00
2024-08-26 17:53:32-07:00	2024-08-26 17:53:35-07:00
2024-08-26 17:53:35-07:00	2024-08-26 17:53:40-07:00
2024-08-26 17:53:17-07:00	2024-08-26 17:53:20-07:00
2024-08-26 17:53:57-07:00	2024-08-26 17:54:02-07:00
2024-08-26 17:54:05-07:00	2024-08-26 17:54:10-07:00
2024-08-26 17:54:10-07:00	2024-08-26 17:54:12-07:00
2024-08-26 17:54:10-07:00	2024-08-26 17:54:12-07:00
2024-08-26 17:54:02-07:00	2024-08-26 17:54:05-07:00
"""

# Split the input string by lines
lines = input_string.strip().split('\n')

# Create a list of tuples
time_intervals = [(line.split("\t")[0], line.split("\t")[1]) for line in lines]

# Connect to the database
conn = init_connection()

# Loop through each interval and generate data for user_id=1
for start_str, end_str in time_intervals:
    start_time = datetime.strptime(start_str,  "%Y-%m-%d %H:%M:%S%z")
    end_time = datetime.strptime(end_str,  "%Y-%m-%d %H:%M:%S%z")
    
    # Generate and insert data for the given interval
    generate_and_insert_data(conn, userid=1, start_time=start_time, end_time=end_time)

# Close the database connection
conn.close()


---
./testing/simulate_arduinodata.py
---
import socket
import time
import random
from datetime import datetime

# Server address and port
UDP_IP = "127.0.0.1"  # Replace with the actual IP address of the UDP server
UDP_IP="74.208.201.225"
UDP_PORT = 8081

# Define the user ID and number of sensors
USER_ID = 1
NUM_SENSORS = 5
MESSAGE_INTERVAL = 1.0 / 50.0  # Interval for 50Hz

def generate_sensor_values(num_sensors):
    """Generate a list of random sensor values."""
    return [random.uniform(0, 1) for _ in range(num_sensors)]

def main():
    # Create a UDP socket
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    millis = 0

    try:
        while True:
            # Generate random sensor values
            sensor_values = generate_sensor_values(NUM_SENSORS)
            
            # Get the current timestamp
            timestamp = datetime.now().isoformat()

            # Create the message string: userid,millis,timestamp,sensor_A0,sensor_A1,...,sensor_A4
            message = f"{USER_ID},{millis}," + ",".join(map(str, sensor_values))
            
            # Send the message to the UDP server
            sock.sendto(message.encode(), (UDP_IP, UDP_PORT))
            print(f"Sent: {message}")
            
            # Increment the millis counter (simulate time progression)
            millis += 20  # Each loop iteration corresponds to 20ms

            # Wait for the next message interval
            time.sleep(MESSAGE_INTERVAL)

    except KeyboardInterrupt:
        print("UDP client stopped.")
    finally:
        sock.close()

if __name__ == "__main__":
    main()


---
./testing/test_inference.py
---
import requests
import psycopg2
import pandas as pd
import json

# Database connection
def get_db_connection():
    return psycopg2.connect(
        host="localhost",
        database="sensordb",
        user="kaavya",
        password=""  # Your password here
    )

# Function to fetch data from user_sensor table in batches
def fetch_sensor_data_batches(user_id, batch_size=64):
    conn = get_db_connection()
    cursor = conn.cursor()

    query = """
    SELECT sensor_a0, sensor_a1, sensor_a2, sensor_a3, sensor_a4
    FROM user_sensor
    WHERE userid = %s
    ORDER BY ts ASC
    LIMIT %s OFFSET %s;
    """

    offset = 0
    while True:
        cursor.execute(query, (user_id, batch_size, offset))
        rows = cursor.fetchall()

        if not rows:
            break

        yield rows
        offset += batch_size

    cursor.close()
    conn.close()

# Function to prepare data for prediction and make API requests
def predict_batches(user_id, api_url):
    for batch in fetch_sensor_data_batches(user_id):
        # Prepare data in the format expected by the API
        data = {
            "userid": user_id,
            "data": [list(row) for row in batch]  # Converting tuples to lists
        }

        # Make POST request to the API
        #print(data)
        response = requests.post(api_url, json=data)

        if response.status_code == 200:
            predictions = response.json().get("predictions", [])
            print(f"Batch predictions: {predictions}")
            print(f"PREDICTIONS:{len(predictions)},ACTUALS:{len(batch)} ")
        else:
            print(f"Error: {response.status_code} - {response.text}")

# Example usage
api_url = "http://localhost:8000/predict"  # Replace with your API URL
user_id = 1
predict_batches(user_id, api_url)


---
./testing/udpsim.py
---
import asyncio
import socket
import random
import time

DEST_IP = "XX.XX.XX.XX"
DEST_PORT = 8081
PACKET_RATE = 50  # packets per second
PACKET_INTERVAL = 1.0 / PACKET_RATE

async def send_udp_packets():
    # Create a UDP socket
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    userid = 2
    packet_count = 0

    # Track the last time we printed
    last_print_time = time.time()

    while True:
        # Get current UTC time in milliseconds since epoch
        elapsed_ms = int(time.time() * 1000)

        # Generate some random sensor values for demonstration
        sensor_values = [
            round(random.uniform(0, 100), 2),  # sensor_A0
            round(random.uniform(0, 100), 2),  # sensor_A1
            round(random.uniform(0, 100), 2),  # sensor_A2
            round(random.uniform(0, 100), 2),  # sensor_A3
            round(random.uniform(0, 100), 2)   # sensor_A4
        ]

        # Construct the comma-separated packet
        # Format: userid,millis,sensorA0,sensorA1,sensorA2,sensorA3,sensorA4
        data_str = f"{userid},{elapsed_ms}," + ",".join(map(str, sensor_values))
        data_bytes = data_str.encode("utf-8")

        # Send the UDP packet
        sock.sendto(data_bytes, (DEST_IP, DEST_PORT))
        packet_count += 1

        # Print once per second to reduce console spam
        current_time = time.time()
        if current_time - last_print_time >= 1.0:
            print(f"Sent packet #{packet_count}: {data_str}")
            last_print_time = current_time

        # Sleep to maintain ~50 packets per second
        await asyncio.sleep(PACKET_INTERVAL)

async def main():
    await send_udp_packets()

if __name__ == "__main__":
    asyncio.run(main())


---
./arduino/arduinocode.ino
---
#include <WiFi.h>
#include <WiFiUdp.h>
#include <Ticker.h>

int status = WL_IDLE_STATUS;

// Please enter your sensitive data in the Secret tab/arduino_secrets.h
char ssid[] = "WIFINAME"; // your network SSID (name)
char pass[] = "WIFIPASSWORD"; // your network password (use for WPA, or use as key for WEP)
int USER_ID = 21;
unsigned int localPort = 2390; // local port to listen on
unsigned int remotePort = 8081
; // remote port to send data to
WiFiUDP Udp;
IPAddress remoteIp; // Define the remote IP address (this should be set to the server's IP)

void setup() {
  remoteIp.fromString("XX.XX.XX.XX"); // Change the IP address here to UDP server
  Serial.begin(115200);

  if (WiFi.status() == WL_NO_MODULE) {
    Serial.println("Communication with WiFi module failed!");
    while (true);
  }

  String fv = WiFi.firmwareVersion();
  if (fv < WIFI_FIRMWARE_LATEST_VERSION) {
    Serial.println("Please upgrade the firmware");
  }

  while (status != WL_CONNECTED) {
    Serial.print("Attempting to connect to SSID: ");
    Serial.println(ssid);
    status = WiFi.begin(ssid, pass);
    Serial.print("WiFi status: ");
    Serial.println(WiFi.status());
    delay(10000);
  }
  Serial.println("Connected to WiFi");
  printWifiStatus();

  Udp.begin(localPort);
}


void loop() {
  // Read sensor values
  int sensorValues[6];
  for (int i = 0; i < 6; i++) {
    sensorValues[i] = analogRead(i);
  }

  // Get the current timestamp
  unsigned long timestamp = millis();

  // Create a character array to hold the data
  char data[100];

  // Format the data as a comma-separated string and get the length
  int length = sprintf(data, "%d,%lu,%d,%d,%d,%d,0", USER_ID, timestamp, sensorValues[0], sensorValues[1], sensorValues[2], sensorValues[3]);

  // Send UDP packet
  Udp.beginPacket(remoteIp, remotePort);
  Udp.write(data, length);
  Udp.endPacket();

  // Print sent data to serial monitor
  Serial.println(data);

  // Wait before sending the next packet
  delay(10);
}

void printWifiStatus() {
  // Print the SSID of the network you're attached to:
  Serial.print("SSID: ");
  Serial.println(WiFi.SSID());

  // Print your board's IP address:
  IPAddress ip = WiFi.localIP();
  Serial.print("IP Address: ");
  Serial.println(ip);

  // Print the received signal strength:
  long rssi = WiFi.RSSI();
  Serial.print("signal strength (RSSI):");
  Serial.print(rssi);
  Serial.println(" dBm");
}

---
./db/postgres.sql
---
-- DROP SCHEMA public;

CREATE ROLE semguser LOGIN PASSWORD 'your_password';

CREATE SCHEMA public AUTHORIZATION semguser;

COMMENT ON SCHEMA public IS 'standard public schema';

-- DROP SEQUENCE api_responses_id_seq;

CREATE SEQUENCE api_responses_id_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE api_responses_id_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE api_responses_id_seq TO semguser;

-- DROP SEQUENCE job_training_metadata_job_training_id_seq;

CREATE SEQUENCE job_training_metadata_job_training_id_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE job_training_metadata_job_training_id_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE job_training_metadata_job_training_id_seq TO semguser;

-- DROP SEQUENCE training_job_artifacts_job_id_seq;

CREATE SEQUENCE training_job_artifacts_job_id_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE training_job_artifacts_job_id_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE training_job_artifacts_job_id_seq TO semguser;

-- DROP SEQUENCE training_job_schedule_job_id_seq;

CREATE SEQUENCE training_job_schedule_job_id_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE training_job_schedule_job_id_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE training_job_schedule_job_id_seq TO semguser;

-- DROP SEQUENCE user_gesture_trainingmetadata_training_metadata_id_seq;

CREATE SEQUENCE user_gesture_trainingmetadata_training_metadata_id_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE user_gesture_trainingmetadata_training_metadata_id_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE user_gesture_trainingmetadata_training_metadata_id_seq TO semguser;

-- DROP SEQUENCE user_gestures_gestureid_seq;

CREATE SEQUENCE user_gestures_gestureid_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE user_gestures_gestureid_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE user_gestures_gestureid_seq TO semguser;

-- DROP SEQUENCE user_sensor_id_seq;

CREATE SEQUENCE user_sensor_id_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE user_sensor_id_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE user_sensor_id_seq TO semguser;

-- DROP SEQUENCE user_video_id_seq;

CREATE SEQUENCE user_video_id_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE user_video_id_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE user_video_id_seq TO semguser;

-- DROP SEQUENCE users_userid_seq;

CREATE SEQUENCE users_userid_seq
	INCREMENT BY 1
	MINVALUE 1
	MAXVALUE 2147483647
	START 1
	CACHE 1
	NO CYCLE;

-- Permissions

ALTER SEQUENCE users_userid_seq OWNER TO semguser;
GRANT ALL ON SEQUENCE users_userid_seq TO semguser;
-- public.user_gesture_trainingmetadata definition

-- Drop table

-- DROP TABLE user_gesture_trainingmetadata;

CREATE TABLE user_gesture_trainingmetadata (
	training_metadata_id serial4 NOT NULL,
	userid int4 NOT NULL,
	gestureid int4 NOT NULL,
	"timestamp" timestamptz NOT NULL,
	sample_number int4 NOT NULL,
	status varchar(50) NOT NULL,
	start_time timestamptz NOT NULL,
	end_time timestamptz NOT NULL,
	CONSTRAINT user_gesture_trainingmetadata_pkey PRIMARY KEY (training_metadata_id)
);

-- Permissions

ALTER TABLE user_gesture_trainingmetadata OWNER TO semguser;
GRANT ALL ON TABLE user_gesture_trainingmetadata TO semguser;


-- public.users definition

-- Drop table

-- DROP TABLE users;

CREATE TABLE users (
	userid serial4 NOT NULL,
	first_name varchar(100) NOT NULL,
	last_name varchar(100) NOT NULL,
	email varchar(150) NOT NULL,
	personal_description text NULL,
	CONSTRAINT users_email_key UNIQUE (email),
	CONSTRAINT users_pkey PRIMARY KEY (userid)
);

-- Permissions

ALTER TABLE users OWNER TO semguser;
GRANT ALL ON TABLE users TO semguser;


-- public.api_responses definition

-- Drop table

-- DROP TABLE api_responses;

CREATE TABLE api_responses (
	id serial4 NOT NULL,
	userid int4 NOT NULL,
	response text NOT NULL,
	"timestamp" timestamptz NOT NULL,
	CONSTRAINT api_responses_pkey PRIMARY KEY (id),
	CONSTRAINT fk_user FOREIGN KEY (userid) REFERENCES users(userid)
);

-- Permissions

ALTER TABLE api_responses OWNER TO semguser;
GRANT ALL ON TABLE api_responses TO semguser;


-- public.training_job_schedule definition

-- Drop table

-- DROP TABLE training_job_schedule;

CREATE TABLE training_job_schedule (
	job_id serial4 NOT NULL,
	userid int4 NULL,
	actual_start_time timestamp NULL,
	actual_end_time timestamp NULL,
	job_status varchar(20) NULL,
	num_samples int4 NULL,
	error_message text NULL,
	log_messages varchar NULL,
	CONSTRAINT training_job_schedule_job_status_check CHECK (((job_status)::text = ANY (ARRAY[('scheduled'::character varying)::text, ('in-progress'::character varying)::text, ('completed'::character varying)::text, ('failed'::character varying)::text, ('canceled'::character varying)::text]))),
	CONSTRAINT training_job_schedule_pkey PRIMARY KEY (job_id),
	CONSTRAINT fk_user FOREIGN KEY (userid) REFERENCES users(userid),
	CONSTRAINT training_job_schedule_userid_fkey FOREIGN KEY (userid) REFERENCES users(userid)
);

-- Permissions

ALTER TABLE training_job_schedule OWNER TO semguser;
GRANT ALL ON TABLE training_job_schedule TO semguser;


-- public.user_gestures definition

-- Drop table

-- DROP TABLE user_gestures;

CREATE TABLE user_gestures (
	gestureid serial4 NOT NULL,
	userid int4 NULL,
	gesture_description text NOT NULL,
	sensor_a0_used bool DEFAULT false NULL,
	sensor_a1_used bool DEFAULT false NULL,
	sensor_a2_used bool DEFAULT false NULL,
	sensor_a3_used bool DEFAULT false NULL,
	sensor_a4_used bool DEFAULT false NULL,
	sensor_a0_purpose text NULL,
	sensor_a1_purpose text NULL,
	sensor_a2_purpose text NULL,
	sensor_a3_purpose text NULL,
	sensor_a4_purpose text NULL,
	CONSTRAINT user_gestures_pkey PRIMARY KEY (gestureid),
	CONSTRAINT user_gestures_userid_fkey FOREIGN KEY (userid) REFERENCES users(userid) ON DELETE CASCADE
);

-- Permissions

ALTER TABLE user_gestures OWNER TO semguser;
GRANT ALL ON TABLE user_gestures TO semguser;


-- public.user_sensor definition

-- Drop table

-- DROP TABLE user_sensor;

CREATE TABLE user_sensor (
	id serial4 NOT NULL,
	userid int4 NULL,
	millis int4 NOT NULL,
	sensor_a0 float4 DEFAULT 0 NULL,
	sensor_a1 float4 DEFAULT 0 NULL,
	sensor_a2 float4 DEFAULT 0 NULL,
	sensor_a3 float4 DEFAULT 0 NULL,
	sensor_a4 float4 DEFAULT 0 NULL,
	ts timestamptz DEFAULT CURRENT_TIMESTAMP NOT NULL,
	CONSTRAINT user_sensor_pkey PRIMARY KEY (id),
	CONSTRAINT user_sensor_userid_fkey FOREIGN KEY (userid) REFERENCES users(userid) ON DELETE CASCADE
);

-- Permissions

ALTER TABLE user_sensor OWNER TO semguser;
GRANT ALL ON TABLE user_sensor TO semguser;


-- public.user_video definition

-- Drop table

-- DROP TABLE user_video;

CREATE TABLE user_video (
	id serial4 NOT NULL,
	userid int4 NULL,
	video_frame bytea NOT NULL,
	"timestamp" timestamptz DEFAULT CURRENT_TIMESTAMP NOT NULL,
	CONSTRAINT user_video_pkey PRIMARY KEY (id),
	CONSTRAINT user_video_userid_fkey FOREIGN KEY (userid) REFERENCES users(userid) ON DELETE CASCADE
);

-- Permissions

ALTER TABLE user_video OWNER TO semguser;
GRANT ALL ON TABLE user_video TO semguser;


-- public.job_training_metadata definition

-- Drop table

-- DROP TABLE job_training_metadata;

CREATE TABLE job_training_metadata (
	job_training_id serial4 NOT NULL,
	job_id int4 NULL,
	training_metadata_id int4 NULL,
	CONSTRAINT job_training_metadata_pkey PRIMARY KEY (job_training_id),
	CONSTRAINT job_training_metadata_job_id_fkey FOREIGN KEY (job_id) REFERENCES training_job_schedule(job_id)
);

-- Permissions

ALTER TABLE job_training_metadata OWNER TO semguser;
GRANT ALL ON TABLE job_training_metadata TO semguser;


-- public.training_job_artifacts definition

-- Drop table

-- DROP TABLE training_job_artifacts;

CREATE TABLE training_job_artifacts (
	job_id serial4 NOT NULL,
	userid int4 NOT NULL,
	model bytea NOT NULL,
	class_mapping jsonb NOT NULL,
	scaler bytea NOT NULL,
	sensors_used jsonb NOT NULL,
	CONSTRAINT training_job_artifacts_pkey PRIMARY KEY (job_id),
	CONSTRAINT training_job_artifacts_job_id_fkey FOREIGN KEY (job_id) REFERENCES training_job_schedule(job_id),
	CONSTRAINT training_job_artifacts_userid_fkey FOREIGN KEY (userid) REFERENCES users(userid)
);

-- Permissions

ALTER TABLE training_job_artifacts OWNER TO semguser;
GRANT ALL ON TABLE training_job_artifacts TO semguser;




-- Permissions

GRANT ALL ON SCHEMA public TO semguser;
GRANT ALL ON SCHEMA public TO public;

---
./db/view_dbs.py
---
import asyncio
import aiosqlite
import glob
import os

async def print_table_info(db_name):
    async with aiosqlite.connect(db_name) as db:
        async with db.execute("SELECT name FROM sqlite_master WHERE type='table';") as cursor:
            print(f"Tables in the database '{db_name}':")
            async for row in cursor:
                table_name = row[0]
                print(f"\nTable: {table_name}")

                # Print the table schema
                async with db.execute(f"PRAGMA table_info({table_name});") as schema_cursor:
                    print("Schema:")
                    async for schema_row in schema_cursor:
                        print(f"  {schema_row[1]} ({schema_row[2]})")
                
                # Print the number of rows
                async with db.execute(f"SELECT COUNT(*) FROM {table_name};") as count_cursor:
                    count_row = await count_cursor.fetchone()
                    print(f"Number of rows: {count_row[0]}")
                
                # Print the last few rows (e.g., the last 5 rows)
                print("Last few rows:")
                async with db.execute(f"SELECT * FROM {table_name} ORDER BY ROWID DESC LIMIT 5;") as data_cursor:
                    async for data_row in data_cursor:
                        print(data_row)

async def main():
    db_files = glob.glob("*.db")  # Find all .db files in the current directory
    if not db_files:
        print("No SQLite database files found in the current directory.")
        return

    for db_file in db_files:
        print(f"\nProcessing database file: {db_file}")
        await print_table_info(db_file)

if __name__ == "__main__":
    asyncio.run(main())


---
./udpserver/udpserver.py
---
import asyncio
import socket
from datetime import datetime
import sys
import psycopg2
from nats.aio.client import Client as NATS

# Initialize NATS client
async def initialize_nats():
    nc = NATS()
    await nc.connect(
        servers=["nats://127.0.0.1:4222"],
        user="XXXXXX",
        password="XXXXXXX"
    )
    return nc

# Initialize PostgreSQL database connection
def initialize_db():
    conn = psycopg2.connect(
        host="localhost",       # Adjust as necessary
        database="sensordb",
        user="XXXXXXX",
        password="XXXXXXXX"
    )
    return conn

# Store sensor data in the PostgreSQL database
def store_data(conn, userid, millis, sensor_values, ts):
    cursor = conn.cursor()
    try:
        # Ensure sensor_values list has 5 elements, filling missing values with 0
        sensor_values += [0] * (5 - len(sensor_values))
        
        cursor.execute('''
            INSERT INTO user_sensor (userid, millis, sensor_a0, sensor_a1, sensor_a2, sensor_a3, sensor_a4, ts)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        ''', (userid, millis, *sensor_values, ts))
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"Failed to store data: {e}")
    finally:
        cursor.close()

# Handle incoming UDP data
async def handle_data(conn, data, addr, nc):
    try:
        # Assuming the data is sent as a comma-separated string: userid,millis,sensor_A0,sensor_A1,sensor_A2,sensor_A3,sensor_A4
        data_str = data.decode().strip()
        values = data_str.split(',')
        
        userid = int(values[0])
        millis = int(values[1])
        sensor_values = [float(value) for value in values[2:]]
        ts = datetime.now().isoformat()  # Calculate timestamp when packet is received

        # Store data in PostgreSQL
        store_data(conn, userid, millis, sensor_values, ts)
        print(f"Stored data from {addr}: {data_str}")
        
        # Publish data to NATS
        message = {
            "userid": userid,
            "millis": millis,
            "sensor_values": sensor_values,
            "timestamp": ts
        }
        await nc.publish("sensor.data", str(message).encode())
        print(f"Published data to NATS: {message}")
    except Exception as e:
        print(f"Failed to handle data from {addr}: {data}. Error: {e}")

# Function to get the IP address of the Wi-Fi LAN
def get_ip_address():
    try:
        # Connect to an external host to find out the network interface IP
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip_address = s.getsockname()[0]
        s.close()
        return ip_address
    except Exception as e:
        return f"Unable to get IP address: {e}"

# Main UDP server function
async def udp_server(host, port, conn, nc):
    loop = asyncio.get_event_loop()
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.bind((host, port))
    sock.setblocking(False)

    ip_address = get_ip_address()
    print(f"Wi-Fi LAN IP Address: {ip_address}")
    print(f"UDP Server started on {ip_address}:{port}")

    while True:
        try:
            data, addr = await loop.run_in_executor(None, sock.recvfrom, 1024)
            asyncio.create_task(handle_data(conn, data, addr, nc))
        except BlockingIOError:
            await asyncio.sleep(0.1)  # Adjust the sleep time as needed

# Main entry point
async def main():
    conn = initialize_db()  # Initialize the PostgreSQL connection
    nc = await initialize_nats()  # Initialize the NATS client
    await udp_server('0.0.0.0', 8081, conn, nc)

if __name__ == "__main__":
    asyncio.run(main())


---
